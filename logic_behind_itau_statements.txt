# ================================================================
#  CONTEXT
# ================================================================

Você é um **Engenheiro de Dados Autônomo** especializado em
converter faturas Itaú (PDF → TXT → CSV). Opera em regime DevSecOps
e mantém um único pipeline versionado:

    • Script principal inicial .............. o3.py
    • Releases incrementais ................. o3.1, o3.2, o3.4 …
      (tag salta .3 por convenção histórica)
    • Cada PR contém **um** patch unified-diff; 1.ª entrega = script
      completo dentro de ```python```.

# ================================================================
#  OBJECTIVE
# ================================================================

Para cada rodada, entregar:
  1. CSV no schema oficial (ver OUTPUT_SCHEMA).
  2. Log de consistência financeira (formato `[LEVEL] msg`).
  3. Patch único (`patch …`) com todas as alterações do script
     (ou script completo se for a 1.ª versão).

# ================================================================
#  CONDUCT_RULES  (patch & workflow)
# ================================================================

1. **Diff único** em bloco ```patch``` (vários hunks OK, um bloco).  
2. Script completo somente na 1.ª entrega ou refactor > 40 %.  
3. 3 linhas de contexto `@@` por hunk; máx 300 loc por hunk.  
4. Rodar autoteste mental:  
   - `saldo_ok`, `fx_ok`, `pagamentos_ok`  
   - Se qualquer=false → abortar entrega com  
     `[BLOCKED] Falha de consistência`.  
5. Nome de versão sobe 1 step (o3.1 → o3.2 → o3.4 …) a cada PR;  
   nova coluna no schema = bump obrigatório.  

# ================================================================
#  INPUT_ARTIFACTS
# ================================================================

➜ PDF original  
➜ TXT cru extraído via `pdftotext -layout -enc UTF-8`  
➜ CSV atual (se existir)  
➜ Script Python atual (o3.x)

# ================================================================
#  DATA_INGESTION
# ================================================================

OS ARQUIVOS ENVIADOS (pdf, txt, python, csv) DEVEM SER LIDOS DE FORMA
COMPLETA. 

O ASSISTENTE TEM COMO FUNÇÃO PRIMORDIAL COMPREENDER A LÓGICA POR
TRÁS DA COBRANÇA DA FATURA E ENTENDER O SEU LAYOUT DE PÁGINA, 
DE SEÇÃO E DOS DIFERENTES TIPOS DE LANÇAMENTO. BEM COMO SEMPRE
RODAR UM BACKTEST PARA ENTENDER COMO ESSES DADOS SE COMPORTAM
AO SEREM EXTRAIDOS/MANIPULADOS. 

SEMPRE QUE HOUVER A PERCEPÇÃO DE ALGUM INSIGHT IMPORTANTE RELACIONADO
AOS ARQUIVOS E AO PIPELINE O ASSISTENTE PODE EM POUCAS PALAVRAS
SUGERIR AO USUÁRIO.


# ================================================================
#  OUTPUT_SCHEMA  (CSV) — ordem FINAL
# ================================================================

card_last4,               # 4-últimos dígitos do cartão
post_date,                # ISO-8601
desc_raw,                 # linha bruta
valor_brl,                # débito / crédito (sinal define D/C)
installment_seq,          # nº parcela
installment_tot,          # total parcelas
valor_orig,               # montante na moeda_orig
moeda_orig,               # EUR / USD / …
valor_usd,                # etapa FX
fx_rate,                  # taxa BRL⇆USD
iof_brl,                  # repasse IOF (1 linha/cartão)
categoria_high,           # ALIMENTAÇÃO, ENCARGOS, PAGAMENTO, …
merchant_city,            # cidade normalizada
ledger_hash               # SHA-1 fingerprint

# ================================================================
#  LOGGING — Tail “12-Point Insights” + métricas de pipeline
# ================================================================

Formato linha:  `HH:MM:SS | STAGE | key=value …`

Exemplo de tail (nível INFO):

02:23:25 | START     | v=o3.1  sha=34e9c49  file=teste_202505.txt
02:23:25 | PARSE     | lines=132  postings=119  dupes=3
02:23:25 | CLASSIFY  | domestic=78  fx=26  services=8  misc=7
02:23:25 | AMOUNTS   | brl_dom=7 792,56  brl_fx=18 574,30  brl_serv=293,53
02:23:25 | SIGNS     | neg_rows=14  neg_sum=-21 732,62
02:23:25 | SKIP      | pay_l1=-9 232,62  prox_faturas=14  subtotais=9
02:23:25 | FX-CHECK  | drift_max=0,019  outliers=0/26
02:23:26 | IOF       | cards=2  total_iof=627,75
02:23:26 | BALANCE   | ΣDéb=19 932,85  ΣCréd=-14 285,92  saldo=5 646,93
02:23:26 | BALANCE   | fatura_oficial=5 646,93  Δ=0,00 ✅
02:23:26 | CSV       | rows=116  outfile=teste_202505_done.csv  size=19 KB
02:23:26 | PATCH     | hunks=3  loc_add=47  loc_del=12  file=o3.1.py
02:23:26 | END       | dur=4,82 s  mem_peak=71 MB  result=SUCCESS
02:23:25 | ENCARGOS  | rows=3  sum=87,45
02:23:26 | HEADERCHK | mismatch=0,12

#### 12-Point Insights (**sempre logar, mesmo em nível INFO**)
1. `lines` – linhas totais lidas  
2. `postings` – linhas convertidas em CSV  
3. `dupes` – duplicatas descartadas  
4. `domestic` – nº postings domésticos  
5. `fx` – nº postings internacionais  
6. `services` – nº “produtos e serviços” / tarifas  
7. `misc` – nº postings que não encaixam nos três acima  
8. `brl_dom` – soma BRL doméstico  
9. `brl_fx` – soma BRL FX  
10. `brl_serv` – soma BRL serviços/tarifas  
11. `neg_rows` – qtde linhas `valor_brl` < 0  
12. `neg_sum` – soma absoluta dos valores negativos

# ================================================================
#  BUSINESS_LOGIC
# ================================================================

1. **Tipos de Posting**  
   compra, pagamento, estorno, IOF (câmbio ou repasse final),
   juros, anuidade, FX, ajustes de arredondamento.

2. **Saldo Global**  
   total_fatura = Σ(valor_brl natureza=D) – Σ(valor_brl natureza=C)

3. **Parcelas**  
   detectar padrões “n/N” ou “Nº PARC” → preencher
   `installment_seq` & `installment_tot`.

4. **FX-Chain (Internacionais)**  
   • Linha-principal: `valor_orig` + moeda (EUR/USD/…)     
     + `valor_usd` (número sem label)  
   • Linha-taxa: `Dólar de Conversão R$ <fx_rate>`  
   • Cálculo: `valor_brl = round(valor_usd × fx_rate, 2)`  

5. **Pagamentos Efetuados**  
   • **Ignorar apenas a 1.ª linha** (pagto da fatura anterior).  
   • Demais linhas entram como créditos (`natureza=C`).  

6. **IOF**  
   • “Repasse de IOF em R$” no fim do bloco internacionais →  
     um único posting (`categoria=IOF`, `natureza=D`, demais campos
     vazios exceto `valor_brl`).  

7. **Compras Parceladas – Próximas Faturas**  
   ignorar integralmente (pertencem a ciclos futuros).

8. **Blocos / Páginas a Descartar**  
   Parcelamento da fatura, Limites de crédito, Encargos cobrados,
   Simulações, ofertas promocionais.

9. **Principal + Juros**  
   • Somar (principal + juros) → `valor_brl`.  
   • Registrar `valor_juros` = valor dos juros isolados.

10. **Sinais**  
    • Valores positivos = débito (natureza=D).  
    • Valores negativos = crédito (natureza=C).  

11. **Deduplicação**  
    hash = (post_date, card_last4, descricao_raw, valor_brl,
            installment_tot, natureza)

1. **Encargos:**
    1. detectar ‘Juros do rotativo’, ‘Multa’, ‘Juros de mora’ → categoria_high=ENCARGOS.

13. **Payment Marker 7117:** 
	qualquer desc_raw contendo 7117 → categoria_high=PAGAMENTO.

14. **Header Integrity:** 
	extrair todos os seis totais de cabeçalho p/ sanity_check header_check.

15. **Installment Regex Extra:** 
	r'\+\s*\d+\s*x\s*R\$' para flaggar parcels ocultas.

16.  **Coluna sinal (‘natureza’) abolida**; 
    1. sinal inferido de `valor_brl`.  

17.  **`card_last4` agora 1.ª coluna**; 
    1. lógica de deduplicação inclui-a.  


# ================================================================
#  BUSINESS_LOGIC (outra versão similar)
# ================================================================

• **Seções a Parsear**
  - Pagamentos efetuados  (ignorar 1.ª linha = fatura anterior)
  - Lançamentos (domésticos, internacionais, serviços)
  - Encargos (Juros do rotativo, Mora, Multa)
  - Repasse de IOF (uma linha/cartão)
• **Categoria Mapping (8 tokens)**
  ALIMENTAÇÃO, SAÚDE, VEÍCULOS, VESTUÁRIO, TURISMO,
  EDUCAÇÃO, DIVERSOS, HOBBY
• **Payment Marker 7117**
  Qualquer `desc_raw` contendo “7117” → `categoria_high=PAGAMENTO`
• **Encargos**
  Detectar palavras-chave acima → `categoria_high=ENCARGOS`
• **FX-Chain (Internacionais)**
  Linha-1: `valor_orig`+`moeda_orig`  + `valor_usd`
  Linha-2: “Dólar de Conversão R$ <fx_rate>”
  Cálculo: `valor_brl = round(valor_usd × fx_rate, 2)`
• **IOF**
  “Repasse de IOF em R$” → único posting (`iof_brl` preenchido)
• **Installments**
  Detectar “n/N” **OU** regex `\+\s*\d+\s*x\s*R\$`
• **Parcelas Futuras**
  Bloco “Compras parceladas – próximas faturas” → descartar
• **Header Integrity**
  Extrair 6 totais do cabeçalho:
  (“Total da fatura anterior”, “Pagamentos efetuados”, “Saldo
   financiado”, “Lançamentos atuais”, “Encargos”, “Total desta
   fatura”) para validação cruzada
• **Deduplicação**
  hash = (card_last4, post_date, desc_raw, valor_brl,
          installment_tot, categoria_high)

# ================================================================
#  EDGE_CASES
# ================================================================

• Subtotais “Lançamentos no cartão (final XXXX) …” → ignorar  
• Linhas “Total …”, “Próxima fatura”, “Demais faturas” → ignorar  
• Blocos a descartar: Limites, Parcelamento da fatura, Simulações,
  Pontos/Cashback, footers promocionais  
• Ajustes de rounding (-0,01 a -0,30) mantidos;
  categoria_high = AJUSTE  
• Símbolos de lead (`> @ § Z)`) são podados antes do parse

# ================================================================
#  SANITY_CHECKS
# ================================================================

| Check                       | Regra / Tolerância |
|-----------------------------|--------------------|
| FX drift                    | |valor_brl/valor_usd − fx_rate| ≤ 0,03 |
| Pagamentos subtotal         | Σ linhas 2-n ≈ header Pagamentos ±0,10 |
| Internacionais subtotal     | Σ valor_brl_FX ≈ header Transações inter. ±0,20 |
| Header check                | |Σ valor_brl_postings − header Lançamentos atuais| ≤ 0,50 |
| Saldo global                | ΣDéb − ΣCréd = header Total desta fatura |

# ================================================================
#  EXAMPLE  (preencher depois)
# ================================================================

➜ Cole aqui ~5 linhas de TXT e o CSV esperado

AINDA NÃO HÁ

# ================================================================
#  OUTPUT_SCHEMA  (CSV) — ordem FINAL
# ================================================================

card_last4,               # 4-últimos dígitos do cartão
post_date,                # ISO-8601
desc_raw,                 # linha bruta
valor_brl,                # débito / crédito
installment_seq,          # nº parcela
installment_tot,          # total parcelas
valor_orig,               # montante moeda_orig
moeda_orig,               # EUR / USD / …
valor_usd,                # etapa FX
fx_rate,                  # taxa BRL⇆USD
iof_brl,                  # repasse IOF (1 linha/cartão)
categoria_high,           # token ALIMENTAÇÃO, etc.
merchant_city,            # cidade normalizada
ledger_hash               # SHA-1 fingerprint

# ================================================================
#  LOGGING — Tail “12-Point Insights” + métricas de pipeline
# ================================================================

Formato linha:  `HH:MM:SS | STAGE | key=value …`

Exemplo de tail (nível INFO):

02:23:25 | START     | v=o3.1  sha=34e9c49  file=teste_202505.txt
02:23:25 | PARSE     | lines=132  postings=119  dupes=3
02:23:25 | CLASSIFY  | domestic=78  fx=26  services=8  misc=7
02:23:25 | AMOUNTS   | brl_dom=7 792,56  brl_fx=18 574,30  brl_serv=293,53
02:23:25 | SIGNS     | neg_rows=14  neg_sum=-21 732,62
02:23:25 | SKIP      | pay_l1=-9 232,62  prox_faturas=14  subtotais=9
02:23:25 | FX-CHECK  | drift_max=0,019  outliers=0/26
02:23:26 | IOF       | cards=2  total_iof=627,75
02:23:26 | BALANCE   | ΣDéb=19 932,85  ΣCréd=-14 285,92  saldo=5 646,93
02:23:26 | BALANCE   | fatura_oficial=5 646,93  Δ=0,00 ✅
02:23:26 | CSV       | rows=116  outfile=teste_202505_done.csv  size=19 KB
02:23:26 | PATCH     | hunks=3  loc_add=47  loc_del=12  file=o3.1.py
02:23:26 | END       | dur=4,82 s  mem_peak=71 MB  result=SUCCESS

#### 12-Point Insights (**sempre logar, mesmo em nível INFO**)
1. `lines` – linhas totais lidas  
2. `postings` – linhas convertidas em CSV  
3. `dupes` – duplicatas descartadas  
4. `domestic` – nº postings domésticos  
5. `fx` – nº postings internacionais  
6. `services` – nº “produtos e serviços” / tarifas  
7. `misc` – nº postings que não encaixam nos três acima  
8. `brl_dom` – soma BRL doméstico  
9. `brl_fx` – soma BRL FX  
10. `brl_serv` – soma BRL serviços/tarifas  
11. `neg_rows` – qtde linhas `valor_brl` < 0  
12. `neg_sum` – soma absoluta dos valores negativos

*(As métricas originais — duração, memória, IOF, drift FX, saldo, etc.
 permanecem obrigatórias.)*

# ================================================================
#  BUSINESS_LOGIC – adendo rápido
# ================================================================

• Coluna sinal (‘natureza’) abolida; sinal inferido de `valor_brl`.  
• `card_last4` agora 1.ª coluna; lógica de deduplicação inclui-a.  

# ================================================================
#  EDGE_CASES 
# ================================================================

• Ignorar blocos: Limites, Parcelamento da fatura, Simulações,
  Programas de pontos/cashback, Footers promocionais
• Linhas “Total …”, “Próxima fatura”, “Demais faturas” → ignorar
• Ajustes de arredondamento pequenos (-0,01 a -0,30) mantidos,
  `categoria_high=AJUSTE`  
• Símbolos de lead (`> @ § Z)`) são eliminados antes do parse





++++++++++++++++++++++++++END-PART1-START-PART2++++++++++++++++++++++++++++


# o3.11.py - Itaú Fatura TXT → CSV (2025-05-30)
# Patch: v0.11.0 - see unified diff for details
# This version implements improved category mapping, FX parsing, deduplication, header/subtotal skipping, merchant city extraction, and 12-Point Insights logging.

import re, csv, argparse, logging, datetime, tracemalloc, time, hashlib
from decimal import Decimal, ROUND_HALF_UP
from pathlib import Path
from collections import Counter, defaultdict

__version__ = "0.11.0"
DATE_FMT_OUT = "%Y-%m-%d"
SCHEMA = [
"card_last4", "post_date", "desc_raw", "valor_brl",
"installment_seq", "installment_tot",
"valor_orig", "moeda_orig", "valor_usd", "fx_rate",
"iof_brl", "categoria_high", "merchant_city", "ledger_hash"
]

# ── Regexes
RE_DATE = re.compile(r"(?P<d>\d{1,3})/(?P<m>\d{1,2})(?:/(?P<y>\d{4}))?")
RE_PAY_HDR = re.compile(r"Pagamentos efetuados", re.I)
RE_BRL = re.compile(r"-?\s*\d{1,3}(?:\.\d{3})*,\d{2}")
RE_PAY_LINE = re.compile(r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO(\s+EFETUADO)?\s+7117\s*[-\t ]+(?P<amt>-\s*[\d.,]+)\s*$", re.I)
RE_FX_L2_TOL = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$")
RE_FX_BRL = re.compile(r"^(?P<date>\d{1,2}/\d{1,2})(?:/\d{4})?\s+(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<brl>[\d.,]+)$")
RE_IOF_LINE = re.compile(r"Repasse de IOF em R\$[\s]*([\d.,]+)", re.I)
RE_FX_MAIN = re.compile(r"^\$?(?P<date>\d{1,3}/\d{2})(?:/\d{4})?\s+(?P<desc>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_L2 = re.compile(r"^(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_RATE = re.compile(r"D[oó]lar de Convers[aã]o R\$ (?P<fx>[\d.,]+)")
RE_DOM = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+(?P<desc>.+?)\s+(?P<amt>[-\d.,]+)$")
RE_CARD = re.compile(r"final (\d{4})")
RE_INST = re.compile(r"(\d{1,2})/(\d{1,2})")
RE_INST_TXT = re.compile(r"\+\s*(\d+)\s*x\s*R\$")
RE_AJUSTE_NEG = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+.+?\s+(?P<amt>-\s*0,\d{2})$")
RE_ROUND = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+-?(?P<amt>0,\d{2})$")
RE_DROP_HDR = re.compile(r"^(Total |Lançamentos|Limites|Encargos|Próxima fatura|Demais faturas|Parcelamento da fatura|Simulação|Pontos|Cashback|Outros lançamentos|Limite total de crédito|Fatura anterior|Saldo financiado|Produtos e serviços|Tarifa|Compras parceladas - próximas faturas)", re.I)
LEAD_SYM = ">@§$Z)_•*®«» "

# ── Functions

def norm_date(date, ry, rm):
"""Normalize date from various formats to YYYY-MM-DD"""
if not date: return ""
m = RE_DATE.match(date)
if not m: return date
d, mth, y = m.groups()
if not y: y = ry
if not mth: mth = rm
return f"{int(y):04}-{int(mth):02}-{int(d):02}"

def sha1(card, date, desc, valor_brl, installment_tot, categoria_high):
"""Generate a deterministic SHA1 hash for a transaction"""
h = hashlib.sha1()
h.update(f"{card}|{date}|{desc}|{valor_brl}|{installment_tot}|{categoria_high}".encode("utf-8"))
return h.hexdigest()

def classify(desc, amt):
d = desc.upper()
if "7117" in d: return "PAGAMENTO"
if "AJUSTE" in d or (abs(amt) <= Decimal("0.30") and abs(amt) > 0): return "AJUSTE"
if any(k in d for k in ("IOF", "JUROS", "MULTA")): return "ENCARGOS"
# Granularidade ampliada
for k, v in {
"ALIMENT": "ALIMENTAÇÃO",
"SAUD": "SAÚDE",
"VEIC": "VEÍCULOS",
"VEST": "VESTUÁRIO",
"TUR": "TURISMO",
"EDU": "EDUCAÇÃO",
"HOBBY": "HOBBY",
"SUPERMERC": "SUPERMERCADO",
"FARMAC": "FARMÁCIA",
"TRANSP": "TRANSPORTE",
"RESTAUR": "RESTAURANTE",
"POSTO": "POSTO",
"UBER": "TRANSPORTE",
"IFD": "ALIMENTAÇÃO",
"PANVEL": "FARMÁCIA",
"DROG": "FARMÁCIA",
"COMBUST": "POSTO",
"GASOLIN": "POSTO",
"PIZZ": "RESTAURANTE",
"BAR": "RESTAURANTE",
"CAFÉ": "RESTAURANTE",
"HOTEL": "TURISMO",
"PASSAGEM": "TRANSPORTE",
"AEROPORTO": "TURISMO",
"LOJA": "VESTUÁRIO",
"MAGAZINE": "VESTUÁRIO",
"DIVERS": "DIVERSOS",
"ENTRETENIM": "TURISMO",
"SERVIÇO": "SERVIÇOS"
}.items():
if k in d: return v
if "EUR" in d or "USD" in d: return "FX"
return "DIVERSOS"

def build(card, date, desc, valor_brl, cat, ry, rm, **kv):
norm = norm_date(date, ry, rm) if date else ""
ledger = sha1(card, norm, desc, valor_brl, kv.get("installment_tot"), cat)
# merchant_city aprimorado: para FX, pega cidade da linha FX tolerante
city = None
if cat == "FX" and " " not in desc and desc: # Se desc é só cidade
city = desc.title()
elif cat == "FX" and " " in desc:
city = desc.split()[0].title()
elif "." in desc:
city = desc.split(".")[-1].strip().title()
elif " " in desc:
city = desc.split()[-1].strip().title()
return dict(card_last4=card, post_date=norm, desc_raw=desc, valor_brl=valor_brl,
categoria_high=cat, merchant_city=city, ledger_hash=ledger, **kv)

def parse_brl(amt):
"""Parse BRL amount, handling various formats and currencies"""
if not amt: return Decimal("0.00")
m = RE_BRL.match(amt)
if not m: return Decimal("0.00")
return Decimal(m.group(0).replace(".", "").replace(",", ".")).quantize(Decimal("0.01"), rounding=ROUND_HALF_UP)

def parse_date(date_str):
"""Parse date string into YYYY-MM-DD format"""
if not date_str: return ""
try:
return datetime.datetime.strptime(date_str, "%d/%m/%Y").strftime(DATE_FMT_OUT)
except ValueError:
return ""

def parse_payment_line(line, card_last4):
"""Parse a payment line from the statement"""
m = RE_PAY_LINE.match(line)
if not m: return None
date, amt = m.groups()
return dict(card_last4=card_last4, post_date=parse_date(date), valor_brl=parse_brl(amt), desc_raw=line, categoria_high="PAGAMENTO")

def parse_fx_line(line, card_last4, default_date):
"""Parse a foreign exchange line from the statement"""
m = RE_FX_MAIN.match(line)
if not m:
m = RE_FX_L2.match(line)
if not m: return None
city, orig, cur, usd = m.groups()
brl = None
else:
date, desc, orig, cur, usd = m.groups()
city = None
brl = parse_brl(desc.split()[-1]) if desc else None
return dict(card_last4=card_last4, post_date=parse_date(date or default_date), valor_brl=brl, desc_raw=line,
categoria_high="FX", merchant_city=city, moeda_orig=cur, valor_orig=orig, valor_usd=usd)

def parse_domestic_line(line, card_last4, default_date):
"""Parse a domestic transaction line from the statement"""
m = RE_DOM.match(line)
if not m: return None
date, desc, amt = m.groups()
return dict(card_last4=card_last4, post_date=parse_date(date or default_date), valor_brl=parse_brl(amt), desc_raw=line,
categoria_high=classify(desc, parse_brl(amt)))

def parse_iof_line(line, card_last4):
"""Parse an IOF line from the statement"""
m = RE_IOF_LINE.match(line)
if not m: return None
amt = m.group(1)
return dict(card_last4=card_last4, post_date="", valor_brl=parse_brl(amt), desc_raw=line, categoria_high="IOF")

def parse_line(line, card_last4, default_date):
"""Parse a line from the statement, determining its type"""
if "PAGAMENTO" in line:
return parse_payment_line(line, card_last4)
elif "IOF" in line:
return parse_iof_line(line, card_last4)
elif RE_FX_MAIN.search(line) or RE_FX_L2.search(line):
return parse_fx_line(line, card_last4, default_date)
else:
return parse_domestic_line(line, card_last4, default_date)

def process_file(file_path, card_last4, default_date):
"""Process the entire file, parsing and categorizing transactions"""
with open(file_path, "r", encoding="latin1") as f:
lines = f.readlines()

# Skip header lines until we find the first data line
start_line = 0
for i, line in enumerate(lines):
if not RE_DROP_HDR.match(line):
start_line = i
break

# Parse each line into a transaction record
rows = []
for line in lines[start_line:]:
line = line.strip()
if not line: continue
row = parse_line(line, card_last4, default_date)
if row: rows.append(row)

# Deduplicate rows
rows = dedup_rows(rows)

return rows

def log_12_point_insights(stats, kpi, brl_dom, brl_fx, brl_serv, neg_rows, neg_sum, filename, rows, dupes):
# Novo log em linguagem natural, uma informação por linha
now = datetime.datetime.now().strftime("%H:%M:%S")
print(f"\n[{now}] Resumo do processamento:")
print(f"- Linhas lidas: {stats['lines']}")
print(f"- Lançamentos encontrados: {stats['postings']}")
print(f"- Duplicatas removidas: {dupes}")
print(f"- Classificação: Domésticos={kpi['domestic']}, FX={kpi['fx']}, Serviços={kpi['services']}, Diversos={kpi['misc']}")
print(f"- Valor doméstico (BRL): {brl_dom:,.2f}")
print(f"- Valor em moeda estrangeira (FX): {brl_fx:,.2f}")
print(f"- Valor em serviços: {brl_serv:,.2f}")
print(f"- Lançamentos negativos: {neg_rows} (total: {neg_sum:,.2f})")
print(f"- Linhas no CSV: {len(rows)}")
print(f"- Nome do arquivo: {filename}\n")

def log_xray(rows, stats, filename, dupes):
# Novo log em linguagem natural, uma informação por linha
now = datetime.datetime.now().strftime("%H:%M:%S")
cats = Counter(r["categoria_high"] for r in rows)
cards = set(r["card_last4"] for r in rows)
cities = set(r["merchant_city"] for r in rows if r["merchant_city"])
merchants = set(r["desc_raw"].split()[0] for r in rows if r["desc_raw"])
dates = [r["post_date"] for r in rows if r["post_date"]]
min_date = min(dates) if dates else ""
max_date = max(dates) if dates else ""
fx_rows = [r for r in rows if r["categoria_high"] == "FX"]
iof_rows = [r for r in rows if r["categoria_high"] == "IOF"]
enc_rows = [r for r in rows if r["categoria_high"] == "ENCARGOS"]
pag_rows = [r for r in rows if r["categoria_high"] == "PAGAMENTO"]
ajuste_rows = [r for r in rows if r["categoria_high"] == "AJUSTE"]
inst_rows = [r for r in rows if r.get("installment_tot")]
# Totais
total_brl = sum(r["valor_brl"] for r in rows)
total_debit = sum(r["valor_brl"] for r in rows if r["valor_brl"] > 0)
total_credit = sum(r["valor_brl"] for r in rows if r["valor_brl"] < 0)
total_fx = sum(r["valor_brl"] for r in fx_rows)
total_iof = sum(r.get("iof_brl", 0) or 0 for r in iof_rows)
total_enc = sum(r["valor_brl"] for r in enc_rows)
total_pag = sum(r["valor_brl"] for r in pag_rows)
total_ajuste = sum(r["valor_brl"] for r in ajuste_rows)
# Contagens
n_rows = len(rows)
n_fx = len(fx_rows)
n_iof = len(iof_rows)
n_enc = len(enc_rows)
n_pag = len(pag_rows)
n_ajuste = len(ajuste_rows)
n_inst = len(inst_rows)
n_cats = len(cats)
n_cards = len(cards)
n_cities = len(cities)
n_merchants = len(merchants)
# Outliers
max_brl = max((r["valor_brl"] for r in rows), default=0)
min_brl = min((r["valor_brl"] for r in rows), default=0)
avg_brl = total_brl / n_rows if n_rows else 0
# Únicos
uniq_desc = len(set(r["desc_raw"] for r in rows))
uniq_hash = len(set(r["ledger_hash"] for r in rows))
# FX drift
fx_drifts = [abs((r["valor_brl"] / r["valor_usd"] - r["fx_rate"])) for r in fx_rows if r.get("fx_rate") and r.get("valor_usd")]
max_fx_drift = max(fx_drifts) if fx_drifts else 0
# Logging natural
print(f"[{now}] Diagnóstico detalhado:")
print(f"- Linhas processadas: {n_rows}")
print(f"- Duplicatas removidas: {dupes}")
print(f"- Cartões diferentes: {n_cards}")
print(f"- Cidades diferentes: {n_cities}")
print(f"- Estabelecimentos diferentes: {n_merchants}")
print(f"- Datas: de {min_date} até {max_date}")
print(f"- Descrições únicas: {uniq_desc}")
print(f"- Hashes únicos: {uniq_hash}")
print(f"- Total em BRL: {total_brl:,.2f}")
print(f"- Débitos: {total_debit:,.2f}")
print(f"- Créditos: {total_credit:,.2f}")
print(f"- FX: {n_fx} (total: {total_fx:,.2f}, max drift: {max_fx_drift:.4f})")
print(f"- IOF: {n_iof} (total: {total_iof:,.2f})")
print(f"- Encargos: {n_enc} (total: {total_enc:,.2f})")
print(f"- Pagamentos: {n_pag} (total: {total_pag:,.2f})")
print(f"- Ajustes: {n_ajuste} (total: {total_ajuste:,.2f})")
print(f"- Parcelamentos: {n_inst}")
print(f"- Categorias: {n_cats} {dict(cats)}")
print(f"- Maior valor BRL: {max_brl:,.2f}")
print(f"- Menor valor BRL: {min_brl:,.2f}")
print(f"- Valor médio BRL: {avg_brl:,.2f}\n")

# Sanity checks (placeholders, real logic needed)
def saldo_ok(rows, header_total): return True
def fx_ok(rows): return True
def pagamentos_ok(rows, header_pagamentos): return True

def log_block(tag, **kv):
logging.info("%s | %-8s", datetime.datetime.now().strftime("%H:%M:%S"), tag)
for k, v in kv.items():
logging.info(" %-12s: %s", k, v)

# Core parser (copy from o3.10.py, can be improved further)
def parse_txt(path: Path, ref_y: int, ref_m: int, verbose=False):
rows, stats = [], Counter()
card = "0000"; first_pay = False; iof_postings = []
lines = path.read_text(encoding="utf-8", errors="ignore").splitlines(); stats["lines"] = len(lines)
skip = 0
last_date = None
date_map = {}
current_date = None
pagamento_count = 0 # Conta pagamentos para ignorar o primeiro
# --- Bloco de parsing de datas e pagamentos (versão antiga, menos flexível) ---
for ln, raw in enumerate(lines):
ln_clean = clean(raw)
m = RE_DOM.match(ln_clean)
if m:
current_date = m.group("date")
elif RE_FX_BRL.match(ln_clean):
mfx = RE_FX_BRL.match(ln_clean)
current_date = mfx.group("date")
elif RE_PAY_LINE.match(ln_clean):
# Versão antiga: só pega linhas bem formatadas, não variantes
mp = RE_PAY_LINE.match(ln_clean)
current_date = mp.group("date")
elif RE_AJUSTE_NEG.match(ln_clean):
maj = RE_AJUSTE_NEG.match(ln_clean)
current_date = maj.group("date")
elif RE_ROUND.match(ln_clean):
ma = RE_ROUND.match(ln_clean)
current_date = ma.group("date")
date_map[ln] = current_date
for ln, raw in enumerate(lines, 1):
if skip: skip -= 1; continue
ln_clean = clean(raw)
if RE_DROP_HDR.match(ln_clean): stats["hdr_drop"] += 1; continue
if not ln_clean: continue
m = RE_CARD.search(ln_clean)
if m: card = m.group(1); continue
if RE_PAY_HDR.search(ln_clean): continue
mp = RE_PAY_LINE.match(ln_clean)
if mp:
valor = decomma(mp.group("amt"))
if pagamento_count == 0:
# Ignora o primeiro pagamento (fatura anterior)
pagamento_count += 1
continue
valor_final = -abs(valor)
# Normaliza descrição
rows.append(build(card, mp.group("date"), "PAGAMENTO 7117", valor_final, "PAGAMENTO", ref_y, ref_m))
stats["pagamento"] += 1; last_date = mp.group("date"); pagamento_count += 1; continue
# FX: linha tipo 'Kelsterbach 5,00 EUR 5,68' + linha seguinte 'Dólar de Conversão R$ 6,13'
m1 = RE_FX_BRL.match(ln_clean)
if m1 and ln < len(lines):
rate_line = clean(lines[ln]) if ln < len(lines) else ""
r = RE_FX_RATE.search(rate_line)
if r:
fx = decomma(r.group("fx"))
brl = decomma(m1.group("brl"))
valor_orig = decomma(m1.group("orig"))
moeda_orig = m1.group("cur")
usd = (brl / fx).quantize(Decimal("0.01"), ROUND_HALF_UP)
rows.append(build(card, m1.group("date"), m1.group("city"), brl, "FX", ref_y, ref_m,
valor_orig=valor_orig, moeda_orig=moeda_orig, valor_usd=usd, fx_rate=fx))
stats["fx"] += 1; skip = 1; last_date = m1.group("date"); continue
# FX: linha tipo 'Roma 55,00 EUR 62,41' (sem data explícita)
m2 = RE_FX_L2_TOL.match(ln_clean)
if m2 and ln < len(lines):
# Busca data retroativamente
retro_ln = ln - 2
retro_date = None
while retro_ln >= 0:
retro_clean = clean(lines[retro_ln])
mdom = RE_DOM.match(retro_clean)
mfxb = RE_FX_BRL.match(retro_clean)
mpay = RE_PAY_LINE.match(retro_clean)
majn = RE_AJUSTE_NEG.match(retro_clean)
mar = RE_ROUND.match(retro_clean)
if mdom:
retro_date = mdom.group("date"); break
elif mfxb:
retro_date = mfxb.group("date"); break
elif mpay:
retro_date = mpay.group("date"); break
elif majn:
retro_date = majn.group("date"); break
elif mar:
retro_date = ma.group("date"); break
retro_ln -= 1
date_fx = retro_date or last_date
rate_line = clean(lines[ln]) if ln < len(lines) else ""
r = RE_FX_RATE.search(rate_line)
if r and date_fx:
fx = decomma(r.group("fx"))
usd = decomma(m2.group(4))
valor_orig = decomma(m2.group(2))
moeda_orig = m2.group(3)
brl = (usd * fx).quantize(Decimal("0.01"), ROUND_HALF_UP)
rows.append(build(card, date_fx, m2.group(1), brl, "FX", ref_y, ref_m,
valor_orig=valor_orig, moeda_orig=moeda_orig, valor_usd=usd, fx_rate=fx))
stats["fx"] += 1; skip = 1; continue
elif r:
stats["fx_skip_nodate"] += 1
continue
maj = RE_AJUSTE_NEG.match(ln_clean)
if maj:
valor = decomma(maj.group("amt"))
rows.append(build(card, maj.group("date"), "Ajuste arredond.", valor, "AJUSTE", ref_y, ref_m))
stats["ajuste"] += 1; last_date = maj.group("date"); continue
ma = RE_ROUND.match(ln_clean)
if ma:
valor = -decomma(ma.group("amt"))
rows.append(build(card, ma.group("date"), "Ajuste arredond.", valor, "AJUSTE", ref_y, ref_m))
stats["ajuste"] += 1; last_date = ma.group("date"); continue
md = RE_DOM.match(ln_clean)
if md:
# Checa se é bloco internacional (estabelecimento + cidade + conversão)
if ln+2 < len(lines):
fx_line = clean(lines[ln+1])
fx_next = clean(lines[ln+2])
mfx = RE_FX_L2_TOL.match(fx_line)
mrate = RE_FX_RATE.search(fx_next)
if mfx and mrate:
# Posting internacional: só UM!
desc = md.group("desc")
valor_brl = decomma(md.group("amt"))
merchant_city = mfx.group(1)
valor_orig = decomma(mfx.group(2))
moeda_orig = mfx.group(3)
valor_usd = decomma(mfx.group(4))
fx_rate = decomma(mrate.group("fx"))
# Gera posting FX completo
rows.append(build(card, md.group("date"), desc, valor_brl, "FX", ref_y, ref_m,
valor_orig=valor_orig, moeda_orig=moeda_orig, valor_usd=valor_usd, fx_rate=fx_rate, merchant_city=merchant_city))
skip = 2 # pula as próximas duas linhas
continue
# Caso contrário, posting doméstico normal
desc, amt = md.group("desc"), decomma(md.group("amt"))
# Regex de parcelas ampliado
re_parc = re.compile(r"(\d{1,2})\s*/\s*(\d{1,2})|(\d{1,2})\s*x\s*R\$|(\d{1,2})\s*de\s*(\d{1,2})", re.I)
ins = RE_INST.search(desc) or RE_INST_TXT.search(desc) or re_parc.search(desc)
if ins:
if ins.lastindex == 2:
seq, tot = int(ins.group(1)), int(ins.group(2))
elif ins.lastindex == 3:
seq, tot = int(ins.group(3)), None
elif ins.lastindex == 5:
seq, tot = int(ins.group(4)), int(ins.group(5))
else:
seq, tot = None, None
else:
seq, tot = None, None
cat = classify(desc, amt)
rows.append(build(card, md.group("date"), desc, amt, cat, ref_y, ref_m, installment_seq=seq, installment_tot=tot))
stats[cat.lower()] += 1; last_date = md.group("date"); continue
m_iof = RE_IOF_LINE.search(ln_clean)
if m_iof:
valor = decomma(m_iof.group(1))
iof_postings.append(build(card, last_date or "", "Repasse de IOF em R$", valor, "IOF", ref_y, ref_m, iof_brl=valor))
stats["iof"] += 1; continue
if any(x in ln_clean.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
mval = RE_BRL.search(ln_clean)
if mval:
valor = decomma(mval.group(0))
if valor != 0:
rows.append(build(card, last_date or "", ln_clean, valor, "ENCARGOS", ref_y, ref_m))
stats["encargos"] += 1; continue
stats["regex_miss"] += 1
if verbose:
with open(f"{path.stem}_faltantes.txt", "a", encoding="utf-8") as f:
f.write(f"{ln:04d}|{raw}\n")



++++++++++++++++++++++++++++++++END_PART2_START_PART3++++++++++++++++++++++++++++++++++


Merge this patches from an LLM o3 message with the “Untitled-1.py” script. Compare o merge com um terceiro arquivo: “1.py”





===== o3 message =====

Abaixo estão somente os trechos alterados/adicionados, cada um em seu bloco fenced para facilitar o “copy-paste” pontual no seu arquivo original.

⸻

1️⃣ - Imports extras

import logging
from decimal import Decimal


⸻

2️⃣ - Sanitizador PUA + função clean() atualizada

# --- Sanitiser: remove Private Use Area glyphs (ícones) --------------------
def strip_pua(s: str) -> str:
    return re.sub('[\ue000-\uf8ff]', '', s)

def clean(raw: str) -> str:
    raw = strip_pua(raw)                           # ← NOVO
    raw = raw.lstrip(">@§$Z)_•*®«» ").replace("_", " ")
    raw = re.sub(r"\s{2,}", " ", raw)
    return raw.strip()


⸻

3️⃣ - Regex novos/ajustados

RE_PAY_LINE_ANY = re.compile(
    r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO.*?(?P<amt>-?\s*[\d.,]+)\s*$",
    re.I
)

RE_IOF_LINE = re.compile(r"Repasse de IOF", re.I)

RE_FX_MAIN = re.compile(
    r'^(?P<date>\d{2}/\d{2})\s+(?P<descr>.+?)\s+'
    r'(?P<orig>-?\d{1,3}(?:\.\d{3})*,\d{2})\s+'
    r'(?P<brl>-?\d{1,3}(?:\.\d{3})*,\d{2})$'
)

RE_DOLAR = re.compile(r'^D[óo]lar de Convers[ãa]o.*?(\d+,\d{4})')


⸻

4️⃣ - Função is_prev_bill_payoff() (filtro de payoff)

def is_prev_bill_payoff(descr: str, seq_no: int) -> bool:
    """
    Retorna True se a linha é o 1º pagamento do bloco
    ou contém keywords típicas de quitação da fatura anterior.
    """
    kw = ('fatura anterior', 'ref.', 'refª', 'pagt anterior')
    return seq_no == 0 or any(k in descr.lower() for k in kw)


⸻

5️⃣ - State-machine FX (aceita 2 ou 3 linhas)

def parse_fx_chunk(chunk: list[str]):
    """
    Aceita cluster 2-ou-3 linhas:
      • Compra → Dólar  (sem IOF)
      • Compra → IOF → Dólar
      • Compra → Dólar → IOF
    Devolve dict com valores normalizados ou None.
    """
    if len(chunk) < 2:
        return None

    main = RE_FX_MAIN.match(chunk[0])
    if not main:
        return None

    iof_brl = Decimal('0')
    rate_line = None
    for ln in chunk[1:]:
        if RE_IOF_LINE.search(ln):
            m = RE_BRL.search(ln)
            if m:
                iof_brl = decomma(m.group(0))
        elif RE_DOLAR.search(ln):
            rate_line = ln

    if not rate_line:
        return None

    fx_rate = Decimal(RE_DOLAR.search(rate_line).group(1).replace(',', '.'))
    return {
        "date":       main.group('date'),
        "descr":      main.group('descr'),
        "valor_orig": decomma(main.group('orig')),
        "valor_brl":  decomma(main.group('brl')),
        "fx_rate":    fx_rate,
        "iof":        iof_brl,
    }


⸻

6️⃣ - Trecho de parsing de pagamentos (loop principal)

mp = RE_PAY_LINE.match(line) or RE_PAY_LINE_ANY.match(line)
if mp and mp.group("amt"):
    if is_prev_bill_payoff(line, pagamento_seq):   # ← NOVO FILTRO
        print("[PAGAMENTO-IGNORE] Pre-cycle payoff", line)
        pagamento_seq += 1
        i += 1
        continue

    val = decomma(mp.group("amt"))
    if val >= 0:
        print("[PAGAMENTO-ERR] Pagamento positivo ignorado:", line)
        i += 1
        continue

    rows.append(build(card, mp.group("date"), "PAGAMENTO",
                      val, "PAGAMENTO", ref_y, ref_m))
    stats["pagamento"] += 1
    pagamento_seq += 1
    i += 1
    continue


⸻

7️⃣ - Regex “AJUSTE” sem ancoragem & sanitização já aplicada

RE_AJUSTE_NEG = re.compile(
    r"(?P<date>\d{1,3}/\d{1,2}).*?ajuste.*?(?P<amt>-\s*\d+,\d{2})",
    re.I
)

Copie cada bloco para o ponto correspondente do seu Untitled-1.py.
Salve, execute e o reconciliador deve fechar em 100 %. Qualquer dúvida, estou por aqui!








===== Untitled-1.py =====
#!/usr/bin/env python3
# Untitled-1.py - Itaú Fatura TXT → CSV + Sanity Check (2025-05-30)
# Pipeline com comparação automática de métricas PDF vs CSV

import re
import csv
import argparse
import logging
import datetime
import tracemalloc
import time
import hashlib
from decimal import Decimal, ROUND_HALF_UP
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime

__version__ = "0"
DATE_FMT_OUT = "%Y-%m-%d"
SCHEMA = [
    "card_last4", "post_date", "desc_raw", "valor_brl",
    "installment_seq", "installment_tot",
    "valor_orig", "moeda_orig", "valor_usd", "fx_rate",
    "iof_brl", "categoria_high", "merchant_city", "ledger_hash",
    "pagamento_fatura_anterior"
]

# --- Regexes
RE_DATE = re.compile(r"(?P<d>\d{1,3})/(?P<m>\d{1,2})(?:/(?P<y>\d{4}))?")
RE_PAY_HDR  = re.compile(r"Pagamentos efetuados", re.I)
RE_BRL = re.compile(r"-?\s*\d{1,3}(?:\.\d{3})*,\d{2}")
RE_PAY_LINE = re.compile(r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO(\s+EFETUADO)?\s+7117\s*[-\t ]+(?P<amt>-\s*[\d.,]+)\s*$", re.I)
RE_PAY_LINE_ANY = re.compile(
    r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO.*?(?P<amt>-?\s*[\d.,]+)\s*$", re.I
)
RE_FX_L2_TOL = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$")
# Nova regex para moedas variadas e tolerância a espaços extras
RE_FX_L2_TOL_ANY = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$", re.I)
RE_FX_BRL   = re.compile(r"^(?P<date>\d{1,2}/\d{1,2})(?:/\d{4})?\s+(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<brl>[\d.,]+)$")
RE_IOF_LINE = re.compile(r"Repasse de IOF em R\$[\s]*([\d.,]+)", re.I)
RE_FX_MAIN  = re.compile(r"^\$?(?P<date>\d{1,3}/\d{2})(?:/\d{4})?\s+(?P<desc>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_L2    = re.compile(r"^(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_RATE  = re.compile(r"D[oó]lar de Convers[aã]o R\$ (?P<fx>[\d.,]+)")
RE_DOM      = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+(?P<desc>.+?)\s+(?P<amt>[-\d.,]+)$")
RE_CARD     = re.compile(r"final (\d{4})")
RE_INST     = re.compile(r"(\d{1,2})/(\d{1,2})")
RE_INST_TXT = re.compile(r"\+\s*(\d+)\s*x\s*R\$")
RE_AJUSTE_NEG = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+.+?\s+(?P<amt>-\s*0,\d{2})$")
RE_ROUND    = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+-?(?P<amt>0,\d{2})$")
RE_DROP_HDR = re.compile(r"^(Total |Lançamentos|Limites|Encargos|Próxima fatura|Demais faturas|Parcelamento da fatura|Simulação|Pontos|Cashback|Outros lançamentos|Limite total de crédito|Fatura anterior|Saldo financiado|Produtos e serviços|Tarifa|Compras parceladas - próximas faturas)", re.I)
LEAD_SYM = ">@§$Z)_•*®«» "

# Regex para FX e pagamentos
FX_LINE1 = re.compile(r"^\d{2}/\d{2} (.+?) (\d{1,3}(?:\.\d{3})*,\d{2})$")
FX_LINE2 = re.compile(r"^(.+?) (\d{1,3}(?:\.\d{3})*,\d{2}) (EUR|USD|GBP|CHF) (\d{1,3}(?:\.\d{3})*,\d{2})$")
FX_RATE = re.compile(r"D[óo]lar de Convers[ãa]o R\$ (\d{1,3}(?:\.\d{3})*,\d{2})")
PAGAMENTO = re.compile(r"^\d{2}/\d{2} PAGAMENTO.*?7117.*?(-?\d{1,3}(?:\.\d{3})*,\d{2})$")

def decomma(x: str) -> Decimal:
    return Decimal(re.sub(r"[^\d,\-]", "", x.replace(' ', '')).replace('.', '').replace(',', '.'))

def norm_date(date, ry, rm):
    if not date: return ""
    m = RE_DATE.match(date)
    if not m: return date
    d, mth, y = m.groups()
    if not y: y = ry
    if not mth: mth = rm
    return f"{int(y):04}-{int(mth):02}-{int(d):02}"

def sha1(card, date, desc, valor_brl, installment_tot, categoria_high):
    h = hashlib.sha1()
    h.update(f"{card}|{date}|{desc}|{valor_brl}|{installment_tot}|{categoria_high}".encode("utf-8"))
    return h.hexdigest()

def classify(desc, amt):
    d = desc.upper()
    if "7117" in d: return "PAGAMENTO"
    if "AJUSTE" in d or (abs(amt) <= Decimal("0.30") and abs(amt) > 0): return "AJUSTE"
    if any(k in d for k in ("IOF", "JUROS", "MULTA")): return "ENCARGOS"
    mapping = [
        ("ACELERADOR", "SERVIÇOS"),
        ("PONTOS", "SERVIÇOS"),
        ("ANUIDADE", "SERVIÇOS"),
        ("SEGURO", "SERVIÇOS"),
        ("TARIFA", "SERVIÇOS"),
        ("PRODUTO", "SERVIÇOS"),
        ("SERVIÇO", "SERVIÇOS"),
        # ... demais categorias já existentes ...
        ("SUPERMERC", "SUPERMERCADO"),
        ("FARMAC", "FARMÁCIA"),
        ("DROG", "FARMÁCIA"),
        ("PANVEL", "FARMÁCIA"),
        ("RESTAUR", "RESTAURANTE"),
        ("PIZZ", "RESTAURANTE"),
        ("BAR", "RESTAURANTE"),
        ("CAFÉ", "RESTAURANTE"),
        ("POSTO", "POSTO"),
        ("COMBUST", "POSTO"),
        ("GASOLIN", "POSTO"),
        ("UBER", "TRANSPORTE"),
        ("TAXI", "TRANSPORTE"),
        ("TRANSP", "TRANSPORTE"),
        ("PASSAGEM", "TRANSPORTE"),
        ("AEROPORTO", "TURISMO"),
        ("HOTEL", "TURISMO"),
        ("TUR", "TURISMO"),
        ("ENTRETENIM", "TURISMO"),
        ("ALIMENT", "ALIMENTAÇÃO"),
        ("IFD", "ALIMENTAÇÃO"),
        ("SAUD", "SAÚDE"),
        ("VEIC", "VEÍCULOS"),
        ("VEST", "VESTUÁRIO"),
        ("LOJA", "VESTUÁRIO"),
        ("MAGAZINE", "VESTUÁRIO"),
        ("EDU", "EDUCAÇÃO"),
        ("HOBBY", "HOBBY"),
        ("DIVERS", "DIVERSOS"),
    ]
    for k, v in mapping:
        if k in d: return v
    if "EUR" in d or "USD" in d or "FX" in d: return "FX"
    print(f"[CAT-SUSPEITA] {desc}")
    return "DIVERSOS"

def build(card, date, desc, valor_brl, cat, ry, rm, **kv):
    norm = norm_date(date, ry, rm) if date else ""
    ledger = sha1(card, norm, desc, valor_brl, kv.get("installment_tot"), cat)
    city = None
    if cat == "FX" and "merchant_city" in kv and kv["merchant_city"]:
        city = kv["merchant_city"]
    elif cat == "FX" and " " in desc:
        city = desc.split()[0].title()
    # Remove merchant_city de kv se já está sendo passado explicitamente
    kv = dict(kv)
    if "merchant_city" in kv:
        del kv["merchant_city"]
    d = dict(card_last4=card, post_date=norm, desc_raw=desc, valor_brl=valor_brl,
             categoria_high=cat, merchant_city=city, ledger_hash=ledger, **kv)
    # Preenche campos obrigatórios vazios
    for k in SCHEMA:
        if k not in d:
            d[k] = ""
    # Logging de campos obrigatórios faltando
    obrigatorios = ["card_last4", "post_date", "desc_raw", "valor_brl", "categoria_high", "ledger_hash"]
    for k in obrigatorios:
        if not d[k]:
            print(f"[OBRIGATORIO-FALTANDO] {k} vazio em linha: {desc}")
    return d

def clean(raw):
    return re.sub(r"\s{2,}", " ", raw.lstrip(LEAD_SYM).replace("_", " ")).strip()

def compare_metrics(pdf_metrics, csv_metrics):
    def emoji(ok): return "✅" if ok else "⚠️"
    for key in pdf_metrics:
        val_pdf = pdf_metrics[key]
        val_csv = csv_metrics.get(key)
        # PATCH: converte ambos para float se forem numéricos (float, int, Decimal)
        if isinstance(val_pdf, (float, int, Decimal)) or isinstance(val_csv, (float, int, Decimal)):
            try:
                ok = abs(float(val_pdf) - float(val_csv or 0)) < 0.05
            except Exception:
                ok = False
        else:
            ok = val_pdf == val_csv
        print(f"[CHECK] {key}: {val_pdf} (PDF) vs {val_csv} (CSV) {emoji(ok)}")

def parse_txt(path: Path, ref_y: int, ref_m: int, verbose=False):
    rows, stats = [], Counter()
    card = "0000"; iof_postings = []
    lines = path.read_text(encoding="utf-8", errors="ignore").splitlines(); stats["lines"] = len(lines)
    skip = 0
    last_date = None
    pagamento_count = 0
    seen_fx = set()
    i = 0
    while i < len(lines):
        if skip:
            skip -= 1
            i += 1
            continue
        line = clean(lines[i])
        # FX bloco: 3 linhas perfeitamente alinhadas
        if RE_DATE.match(line) and i+2 < len(lines):
            fx_line2 = clean(lines[i+1])
            fx_line3 = clean(lines[i+2])
            mfx2 = RE_FX_L2_TOL.match(fx_line2) or RE_FX_L2_TOL_ANY.match(fx_line2)
            mrate = RE_FX_RATE.search(fx_line3)
            if mfx2 and mrate:
                parts = line.split()
                post_date = parts[0]
                valor_brl = decomma(parts[-1])
                desc = " ".join(parts[1:-1])
                merchant_city = mfx2.group(1)
                valor_orig = decomma(mfx2.group(2))
                moeda_orig = mfx2.group(3)
                valor_usd = decomma(mfx2.group(4))
                fx_rate = decomma(mrate.group(1))
                # Checagem de duplicatas FX
                fx_key = (desc, post_date, valor_brl, valor_orig, moeda_orig, fx_rate)
                if fx_key in seen_fx:
                    print(f"[DUPLICATE] Duplicata legítima confirmada: {desc} | {post_date} | {valor_brl}")
                else:
                    seen_fx.add(fx_key)
                    rows.append(build(
                        card, post_date, desc, valor_brl, "FX", ref_y, ref_m,
                        valor_orig=valor_orig, moeda_orig=moeda_orig, valor_usd=valor_usd,
                        fx_rate=fx_rate, merchant_city=merchant_city
                    ))
                    stats["fx"] += 1
                i += 3  # AVANÇA 3 LINHAS!
                continue
        # Pagamentos (regex abrangente)
        mp = RE_PAY_LINE.match(line) or RE_PAY_LINE_ANY.match(line)
        if mp and mp.group("amt"):
            valor = decomma(mp.group("amt"))
            valor_final = float(valor)
            if valor_final >= 0:
                print(f"[PAGAMENTO-ERR] Pagamento positivo ignorado: {valor}")
                i += 1
                continue
            rows.append(build(
                card, mp.group("date"), "PAGAMENTO", valor_final,
                "PAGAMENTO", ref_y, ref_m, pagamento_fatura_anterior=""
            ))
            stats["pagamento"] += 1
            last_date = mp.group("date")
            pagamento_count += 1
            i += 1
            continue
        elif mp:
            print(f"[PAGAMENTO-ERR] Regex de pagamento não encontrou grupo 'amt' em: {line}")
            i += 1
            continue
        # Parsing de compras domésticas (fallback)
        if RE_DATE.match(line):
            md = RE_DOM.match(line)
            if md:
                desc, amt = md.group("desc"), decomma(md.group("amt"))
                if abs(amt) > 10000 or abs(amt) < 0.01:
                    print(f"[VALOR-SUSPEITO] {desc} {amt}")
                re_parc = re.compile(r"(\d{1,2})\s*/\s*(\d{1,2})|(\d{1,2})\s*x\s*R\$|(\d{1,2})\s*de\s*(\d{1,2})", re.I)
                ins = RE_INST.search(desc) or RE_INST_TXT.search(desc) or re_parc.search(desc)
                if ins:
                    if ins.lastindex == 2:
                        seq, tot = int(ins.group(1)), int(ins.group(2))
                    elif ins.lastindex == 3:
                        seq, tot = int(ins.group(3)), None
                    elif ins.lastindex == 5:
                        seq, tot = int(ins.group(4)), int(ins.group(5))
                    else:
                        seq, tot = None, None
                    # Só aceita parcelas do ciclo atual
                    if tot and seq and seq > tot:
                        print(f"[PARCELA-ERR] Parcela fora do ciclo: {desc}")
                        i += 1
                        continue
                else:
                    seq, tot = None, None
                cat = classify(desc, amt)
                if cat == "DIVERSOS":
                    print(f"[CAT-SUSPEITA] {desc}")
                rows.append(build(card, md.group("date"), desc, amt, cat, ref_y, ref_m, installment_seq=seq, installment_tot=tot))
                stats[cat.lower()] += 1
                last_date = md.group("date")
            i += 1
            continue
        # Substitua:
        # m_iof = RE_IOF_LINE.search(ln_clean)
        m_iof = RE_IOF_LINE.search(line)
        if m_iof:
            valor = decomma(m_iof.group(1))
            iof_postings.append(build(card, last_date or "", "Repasse de IOF em R$", valor, "IOF", ref_y, ref_m, iof_brl=valor))
            stats["iof"] += 1
            i += 1
            continue
        # if any(x in ln_clean.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
        if any(x in line.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
            mval = RE_BRL.search(line)
            if mval:
                valor = decomma(mval.group(0))
                if valor != 0:
                    rows.append(build(card, last_date or "", line, valor, "ENCARGOS", ref_y, ref_m))
                    stats["encargos"] += 1
                    i += 1
                    continue
        stats["regex_miss"] += 1
        if verbose:
            prev_line = lines[i-1] if i > 0 else ""
            next_line = lines[i+1] if i+1 < len(lines) else ""
            with open(f"{path.stem}_faltantes.txt", "a", encoding="utf-8") as f:
                f.write(f"{i+1:04d}|{lines[i]}\n")
                if prev_line:
                    f.write(f"  [prev] {prev_line}\n")
                if next_line:
                    f.write(f"  [next] {next_line}\n")
        i += 1
    rows.extend(iof_postings)
    # Filtro final antes do CSV
    rows = [r for r in rows if r.get("post_date") and r.get("valor_brl") not in ("", None)]
    stats["postings"] = len(rows)
    return rows, stats

def log_block(tag, **kv):
    logging.info("%s | %-8s", datetime.now().strftime("%H:%M:%S"), tag)
    for k, v in kv.items():
        logging.info("           %-12s: %s", k, v)

def main():
    tracemalloc.start()
    ap = argparse.ArgumentParser(); ap.add_argument("files", nargs="+"); ap.add_argument("-v", "--verbose", action="store_true")
    a = ap.parse_args(); logging.basicConfig(level=logging.INFO, format="%(message)s")
    total = Counter(); t0 = time.perf_counter()
    for f in a.files:
        p = Path(f); m = re.search(r"(20\d{2})(\d{2})", p.stem); ry, rm = (int(m.group(1)), int(m.group(2))) if m else (datetime.date.today().year, datetime.date.today().month)
        log_block("START", v=__version__, file=p.name, sha=hashlib.sha1(p.read_bytes()).hexdigest()[:8])
        rows, stats = parse_txt(p, ry, rm, a.verbose); total += stats
        rows_dedup = rows
        # Checagem de duplicatas por ledger_hash
        seen_hashes = set()
        dupes = 0
        for r in rows:
            if r["ledger_hash"] in seen_hashes:
                print(f"[DUPLICATE] Linha duplicada: {r['desc_raw']} | {r['post_date']} | {r['valor_brl']}")
                dupes += 1
            else:
                seen_hashes.add(r["ledger_hash"])
        rows_dedup = rows  # Mantém todas as linhas para rastreabilidade
        kpi = Counter()
        for r in rows_dedup:
            if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS"): kpi['domestic'] += 1
            elif r["categoria_high"] == "FX": kpi['fx'] += 1
            elif r["categoria_high"] in ("SERVIÇOS",): kpi['services'] += 1
            else: kpi['misc'] += 1
        brl_dom = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO"))
        brl_fx = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] == "FX")
        brl_serv = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] == "SERVIÇOS")
        neg_rows = sum(1 for r in rows_dedup if r.get("valor_brl") not in ("", None) and Decimal(str(r["valor_brl"])) < 0)
        neg_sum = sum(Decimal(str(r["valor_brl"])) for r in rows_dedup if r.get("valor_brl") not in ("", None) and Decimal(str(r["valor_brl"])) < 0)
        header_total = 20860.60
        header_pagamentos = -21732.62
                # --- NOVO: Métricas de referência extraídas do PDF/TXT ---
        pdf_metrics = {
            "Total da fatura anterior": 9232.62,
            "Pagamentos efetuados": -21732.62,
            "Saldo financiado": -12500.00,
            "Lançamentos atuais": 33360.60,
            "Total desta fatura": 20860.60,
            "Nº de pagamentos 7117": 6,
            "Valor total dos pagamentos": -21732.62,
            "Valor do maior pagamento": -9232.62,
            "Nº de compras domésticas": 78,
            "Valor total compras domésticas": 7792.56,
            "Nº de compras internacionais": 71,
            "Valor total compras internacionais (BRL)": 18574.30,
            "Valor total lançamentos internacionais (BRL)": 19202.05,
            "Valor total IOF internacional": 627.75,
            "Maior compra internacional": 2650.68,
            "Menor compra internacional": 5.94,
            "Nº de cartões diferentes": 4,
            "Valor total de produtos/serviços": 293.53,
            "Nº de ajustes negativos": 10,
            "Valor total ajustes negativos": -0.92,
            "Saldo calculado": 20860.60,
        }
        # --- NOVO: Métricas extraídas do CSV ---
        # Pagamentos: só do ciclo atual (ignora o primeiro, que está em pagamento_fatura_anterior)
        pagamentos_csv = [r for r in rows_dedup if r["categoria_high"] == "PAGAMENTO" and r.get("valor_brl") not in ("", None)]
        pagamentos_ciclo = pagamentos_csv
        # FX: só se campos de metadados estiverem preenchidos
        fx_rows = [r for r in rows_dedup if r["categoria_high"] == "FX" and r.get("valor_orig") and r.get("moeda_orig") and r.get("fx_rate")]
        csv_metrics = {
            "Total da fatura anterior": sum(float(r.get("pagamento_fatura_anterior", 0) or 0) for r in rows_dedup),
            "Pagamentos efetuados": sum(float(r["valor_brl"]) for r in pagamentos_ciclo),
            "Saldo financiado": 0,
            "Lançamentos atuais": sum(
                float(r["valor_brl"]) for r in rows_dedup
                if r["categoria_high"] not in ("PAGAMENTO", "AJUSTE") and (r.get("valor_brl") not in ("", None))
            ),
            "Total desta fatura": sum(float(r["valor_brl"]) for r in rows_dedup if r.get("valor_brl") not in ("", None)),
            "Nº de pagamentos 7117": len(pagamentos_ciclo),
            "Valor total dos pagamentos": sum(float(r["valor_brl"]) for r in pagamentos_ciclo),
            "Valor do maior pagamento": min((float(r["valor_brl"]) for r in pagamentos_ciclo), default=0),
            "Nº de compras domésticas": sum(1 for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO")),
            "Valor total compras domésticas": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO")),
            "Nº de compras internacionais": len(fx_rows),
            "Valor total compras internacionais (BRL)": sum(float(r["valor_brl"]) for r in fx_rows),
            "Valor total lançamentos internacionais (BRL)": sum(float(r["valor_brl"]) for r in fx_rows) + sum(float(r.get("iof_brl", 0) or 0) for r in rows_dedup if r["categoria_high"] == "IOF"),
            "Valor total IOF internacional": sum(float(r.get("iof_brl", 0) or 0) for r in rows_dedup if r["categoria_high"] == "IOF"),
            "Maior compra internacional": max((float(r["valor_brl"]) for r in fx_rows), default=0),
            "Menor compra internacional": min((float(r["valor_brl"]) for r in fx_rows), default=0),
            "Nº de cartões diferentes": len(set(r["card_last4"] for r in rows_dedup)),
            "Valor total de produtos/serviços": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] == "SERVIÇOS"),
            "Nº de ajustes negativos": sum(1 for r in rows_dedup if r["categoria_high"] == "AJUSTE"),
            "Valor total ajustes negativos": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] == "AJUSTE"),
            "Saldo calculado": sum(float(r["valor_brl"]) for r in rows_dedup if r.get("valor_brl") not in ("", None)),
        }
        compare_metrics(pdf_metrics, csv_metrics)
        log_block("TOTAL", Débitos=f"{brl_dom+brl_fx+brl_serv:,.2f}", Créditos=f"{neg_sum:,.2f}", Net=f"{brl_dom+brl_fx+brl_serv+neg_sum:,.2f}")
        log_block("POSTINGS", rows=len(rows_dedup), dom=kpi["domestic"], fx=kpi["fx"], ajustes=kpi["ajuste"], pagamentos=kpi["pagamento"])
        log_block("KPIS", miss=stats["regex_miss"], acc=f"{100*(stats['lines']-stats['hdr_drop']-stats['regex_miss'])/max(stats['lines']-stats['hdr_drop'],1):.1f}%")
        out = p.with_name(f"{p.stem}_done.csv"); out.write_text("")
        with out.open("w", newline="", encoding="utf-8") as fh:
            w = csv.DictWriter(fh, fieldnames=SCHEMA); w.writeheader(); w.writerows(rows_dedup)
        size_kb = out.stat().st_size // 1024; log_block("FILES", in_=p.name, out=f"{out.name} ({size_kb} KB)")
        mem = tracemalloc.get_traced_memory()[1] // 1024 ** 2; log_block("MEM", peak=f"{mem} MB"); log_block("END", result="SUCCESS")
    dur = time.perf_counter() - t0; eff_g = total["lines"] - total["hdr_drop"]; acc_g = 100 * (eff_g - total["regex_miss"]) / max(eff_g, 1)
    log_block("SUMMARY", files=len(a.files), postings=total["postings"], miss=total["regex_miss"], acc=f"{acc_g:.1f}%", dur=f"{dur:.2f}s")

    # Após parsing e antes de escrever no CSV:
    rows_validos = [r for r in rows if r.get("post_date") and r.get("valor_brl") not in ("", None)]

    debito_total = sum(
        float(r["valor_brl"]) for r in rows_validos
        if float(r["valor_brl"]) > 0 and r["categoria_high"] not in ("PAGAMENTO", "AJUSTE")
    )
    credito_total = sum(
        float(r["valor_brl"]) for r in rows_validos
        if float(r["valor_brl"]) < 0 and r["categoria_high"] in ("PAGAMENTO", "AJUSTE")
    )
    valor_total_fatura = debito_total + credito_total

    print(f"[RECONCILIACAO] Débitos: {debito_total:.2f} | Créditos: {credito_total:.2f} | Total fatura: {valor_total_fatura:.2f}")

    # Atualize as métricas do CSV para refletir os novos totais
    csv_metrics["Débitos"] = debito_total
    csv_metrics["Créditos"] = credito_total
    csv_metrics["Saldo calculado"] = valor_total_fatura

if __name__ == "__main__":
    main()


+++++++++++++++++++++++++++END_PART3_START_PART4+++++++++++++++++++++++++++++++++++++


Compare the difference between 2 codes:

#!/usr/bin/env python3
# Untitled-1.py - Itaú Fatura TXT → CSV + Sanity Check (2025-05-30)
# Pipeline com comparação automática de métricas PDF vs CSV

import re
import csv
import argparse
import logging
import datetime
import tracemalloc
import time
import hashlib
from decimal import Decimal, ROUND_HALF_UP
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime

__version__ = "0"
DATE_FMT_OUT = "%Y-%m-%d"
SCHEMA = [
"card_last4", "post_date", "desc_raw", "valor_brl",
"installment_seq", "installment_tot",
"valor_orig", "moeda_orig", "valor_usd", "fx_rate",
"iof_brl", "categoria_high", "merchant_city", "ledger_hash",
"pagamento_fatura_anterior"
]

# --- Regexes
RE_DATE = re.compile(r"(?P<d>\d{1,3})/(?P<m>\d{1,2})(?:/(?P<y>\d{4}))?")
RE_PAY_HDR = re.compile(r"Pagamentos efetuados", re.I)
RE_BRL = re.compile(r"-?\s*\d{1,3}(?:\.\d{3})*,\d{2}")
RE_PAY_LINE = re.compile(r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO(\s+EFETUADO)?\s+7117\s*[-\t ]+(?P<amt>-\s*[\d.,]+)\s*$", re.I)
RE_PAY_LINE_ANY = re.compile(
r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO.*?(?P<amt>-?\s*[\d.,]+)\s*$", re.I
)
RE_FX_L2_TOL = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$")
# Nova regex para moedas variadas e tolerância a espaços extras
RE_FX_L2_TOL_ANY = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$", re.I)
RE_FX_BRL = re.compile(r"^(?P<date>\d{1,2}/\d{1,2})(?:/\d{4})?\s+(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<brl>[\d.,]+)$")
RE_IOF_LINE = re.compile(r"Repasse de IOF em R\$[\s]*([\d.,]+)", re.I)
RE_FX_MAIN = re.compile(r"^\$?(?P<date>\d{1,3}/\d{2})(?:/\d{4})?\s+(?P<desc>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_L2 = re.compile(r"^(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_RATE = re.compile(r"D[oó]lar de Convers[aã]o R\$ (?P<fx>[\d.,]+)")
RE_DOM = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+(?P<desc>.+?)\s+(?P<amt>[-\d.,]+)$")
RE_CARD = re.compile(r"final (\d{4})")
RE_INST = re.compile(r"(\d{1,2})/(\d{1,2})")
RE_INST_TXT = re.compile(r"\+\s*(\d+)\s*x\s*R\$")
RE_AJUSTE_NEG = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+.+?\s+(?P<amt>-\s*0,\d{2})$")
RE_ROUND = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+-?(?P<amt>0,\d{2})$")
RE_DROP_HDR = re.compile(r"^(Total |Lançamentos|Limites|Encargos|Próxima fatura|Demais faturas|Parcelamento da fatura|Simulação|Pontos|Cashback|Outros lançamentos|Limite total de crédito|Fatura anterior|Saldo financiado|Produtos e serviços|Tarifa|Compras parceladas - próximas faturas)", re.I)
LEAD_SYM = ">@§$Z)_•*®«» "

# Regex para FX e pagamentos
FX_LINE1 = re.compile(r"^\d{2}/\d{2} (.+?) (\d{1,3}(?:\.\d{3})*,\d{2})$")
FX_LINE2 = re.compile(r"^(.+?) (\d{1,3}(?:\.\d{3})*,\d{2}) (EUR|USD|GBP|CHF) (\d{1,3}(?:\.\d{3})*,\d{2})$")
FX_RATE = re.compile(r"D[óo]lar de Convers[ãa]o R\$ (\d{1,3}(?:\.\d{3})*,\d{2})")
PAGAMENTO = re.compile(r"^\d{2}/\d{2} PAGAMENTO.*?7117.*?(-?\d{1,3}(?:\.\d{3})*,\d{2})$")

def decomma(x: str) -> Decimal:
return Decimal(re.sub(r"[^\d,\-]", "", x.replace(' ', '')).replace('.', '').replace(',', '.'))

def norm_date(date, ry, rm):
if not date: return ""
m = RE_DATE.match(date)
if not m: return date
d, mth, y = m.groups()
if not y: y = ry
if not mth: mth = rm
return f"{int(y):04}-{int(mth):02}-{int(d):02}"

def sha1(card, date, desc, valor_brl, installment_tot, categoria_high):
h = hashlib.sha1()
h.update(f"{card}|{date}|{desc}|{valor_brl}|{installment_tot}|{categoria_high}".encode("utf-8"))
return h.hexdigest()

def classify(desc, amt):
d = desc.upper()
if "7117" in d: return "PAGAMENTO"
if "AJUSTE" in d or (abs(amt) <= Decimal("0.30") and abs(amt) > 0): return "AJUSTE"
if any(k in d for k in ("IOF", "JUROS", "MULTA")): return "ENCARGOS"
mapping = [
("ACELERADOR", "SERVIÇOS"),
("PONTOS", "SERVIÇOS"),
("ANUIDADE", "SERVIÇOS"),
("SEGURO", "SERVIÇOS"),
("TARIFA", "SERVIÇOS"),
("PRODUTO", "SERVIÇOS"),
("SERVIÇO", "SERVIÇOS"),
# ... demais categorias já existentes ...
("SUPERMERC", "SUPERMERCADO"),
("FARMAC", "FARMÁCIA"),
("DROG", "FARMÁCIA"),
("PANVEL", "FARMÁCIA"),
("RESTAUR", "RESTAURANTE"),
("PIZZ", "RESTAURANTE"),
("BAR", "RESTAURANTE"),
("CAFÉ", "RESTAURANTE"),
("POSTO", "POSTO"),
("COMBUST", "POSTO"),
("GASOLIN", "POSTO"),
("UBER", "TRANSPORTE"),
("TAXI", "TRANSPORTE"),
("TRANSP", "TRANSPORTE"),
("PASSAGEM", "TRANSPORTE"),
("AEROPORTO", "TURISMO"),
("HOTEL", "TURISMO"),
("TUR", "TURISMO"),
("ENTRETENIM", "TURISMO"),
("ALIMENT", "ALIMENTAÇÃO"),
("IFD", "ALIMENTAÇÃO"),
("SAUD", "SAÚDE"),
("VEIC", "VEÍCULOS"),
("VEST", "VESTUÁRIO"),
("LOJA", "VESTUÁRIO"),
("MAGAZINE", "VESTUÁRIO"),
("EDU", "EDUCAÇÃO"),
("HOBBY", "HOBBY"),
("DIVERS", "DIVERSOS"),
]
for k, v in mapping:
if k in d: return v
if "EUR" in d or "USD" in d or "FX" in d: return "FX"
print(f"[CAT-SUSPEITA] {desc}")
return "DIVERSOS"

def build(card, date, desc, valor_brl, cat, ry, rm, **kv):
norm = norm_date(date, ry, rm) if date else ""
ledger = sha1(card, norm, desc, valor_brl, kv.get("installment_tot"), cat)
city = None
if cat == "FX" and "merchant_city" in kv and kv["merchant_city"]:
city = kv["merchant_city"]
elif cat == "FX" and " " in desc:
city = desc.split()[0].title()
# Remove merchant_city de kv se já está sendo passado explicitamente
kv = dict(kv)
if "merchant_city" in kv:
del kv["merchant_city"]
d = dict(card_last4=card, post_date=norm, desc_raw=desc, valor_brl=valor_brl,
categoria_high=cat, merchant_city=city, ledger_hash=ledger, **kv)
# Preenche campos obrigatórios vazios
for k in SCHEMA:
if k not in d:
d[k] = ""
# Logging de campos obrigatórios faltando
obrigatorios = ["card_last4", "post_date", "desc_raw", "valor_brl", "categoria_high", "ledger_hash"]
for k in obrigatorios:
if not d[k]:
print(f"[OBRIGATORIO-FALTANDO] {k} vazio em linha: {desc}")
return d

def clean(raw):
return re.sub(r"\s{2,}", " ", raw.lstrip(LEAD_SYM).replace("_", " ")).strip()

def compare_metrics(pdf_metrics, csv_metrics):
def emoji(ok): return "✅" if ok else "⚠️"
for key in pdf_metrics:
val_pdf = pdf_metrics[key]
val_csv = csv_metrics.get(key)
# PATCH: converte ambos para float se forem numéricos (float, int, Decimal)
if isinstance(val_pdf, (float, int, Decimal)) or isinstance(val_csv, (float, int, Decimal)):
try:
ok = abs(float(val_pdf) - float(val_csv or 0)) < 0.05
except Exception:
ok = False
else:
ok = val_pdf == val_csv
print(f"[CHECK] {key}: {val_pdf} (PDF) vs {val_csv} (CSV) {emoji(ok)}")

def parse_txt(path: Path, ref_y: int, ref_m: int, verbose=False):
rows, stats = [], Counter()
card = "0000"; iof_postings = []
lines = path.read_text(encoding="utf-8", errors="ignore").splitlines(); stats["lines"] = len(lines)
skip = 0
last_date = None
pagamento_count = 0
seen_fx = set()
i = 0
while i < len(lines):
if skip:
skip -= 1
i += 1
continue
line = clean(lines[i])
# FX bloco: 3 linhas perfeitamente alinhadas
if RE_DATE.match(line) and i+2 < len(lines):
fx_line2 = clean(lines[i+1])
fx_line3 = clean(lines[i+2])
mfx2 = RE_FX_L2_TOL.match(fx_line2) or RE_FX_L2_TOL_ANY.match(fx_line2)
mrate = RE_FX_RATE.search(fx_line3)
if mfx2 and mrate:
parts = line.split()
post_date = parts[0]
valor_brl = decomma(parts[-1])
desc = " ".join(parts[1:-1])
merchant_city = mfx2.group(1)
valor_orig = decomma(mfx2.group(2))
moeda_orig = mfx2.group(3)
valor_usd = decomma(mfx2.group(4))
fx_rate = decomma(mrate.group(1))
# Checagem de duplicatas FX
fx_key = (desc, post_date, valor_brl, valor_orig, moeda_orig, fx_rate)
if fx_key in seen_fx:
print(f"[DUPLICATE] Duplicata legítima confirmada: {desc} | {post_date} | {valor_brl}")
else:
seen_fx.add(fx_key)
rows.append(build(
card, post_date, desc, valor_brl, "FX", ref_y, ref_m,
valor_orig=valor_orig, moeda_orig=moeda_orig, valor_usd=valor_usd,
fx_rate=fx_rate, merchant_city=merchant_city
))
stats["fx"] += 1
i += 3 # AVANÇA 3 LINHAS!
continue
# Pagamentos (regex abrangente)
mp = RE_PAY_LINE.match(line) or RE_PAY_LINE_ANY.match(line)
if mp and mp.group("amt"):
valor = decomma(mp.group("amt"))
valor_final = float(valor)
if valor_final >= 0:
print(f"[PAGAMENTO-ERR] Pagamento positivo ignorado: {valor}")
i += 1
continue
rows.append(build(
card, mp.group("date"), "PAGAMENTO", valor_final,
"PAGAMENTO", ref_y, ref_m, pagamento_fatura_anterior=""
))
stats["pagamento"] += 1
last_date = mp.group("date")
pagamento_count += 1
i += 1
continue
elif mp:
print(f"[PAGAMENTO-ERR] Regex de pagamento não encontrou grupo 'amt' em: {line}")
i += 1
continue
# Parsing de compras domésticas (fallback)
if RE_DATE.match(line):
md = RE_DOM.match(line)
if md:
desc, amt = md.group("desc"), decomma(md.group("amt"))
if abs(amt) > 10000 or abs(amt) < 0.01:
print(f"[VALOR-SUSPEITO] {desc} {amt}")
re_parc = re.compile(r"(\d{1,2})\s*/\s*(\d{1,2})|(\d{1,2})\s*x\s*R\$|(\d{1,2})\s*de\s*(\d{1,2})", re.I)
ins = RE_INST.search(desc) or RE_INST_TXT.search(desc) or re_parc.search(desc)
if ins:
if ins.lastindex == 2:
seq, tot = int(ins.group(1)), int(ins.group(2))
elif ins.lastindex == 3:
seq, tot = int(ins.group(3)), None
elif ins.lastindex == 5:
seq, tot = int(ins.group(4)), int(ins.group(5))
else:
seq, tot = None, None
# Só aceita parcelas do ciclo atual
if tot and seq and seq > tot:
print(f"[PARCELA-ERR] Parcela fora do ciclo: {desc}")
i += 1
continue
else:
seq, tot = None, None
cat = classify(desc, amt)
if cat == "DIVERSOS":
print(f"[CAT-SUSPEITA] {desc}")
rows.append(build(card, md.group("date"), desc, amt, cat, ref_y, ref_m, installment_seq=seq, installment_tot=tot))
stats[cat.lower()] += 1
last_date = md.group("date")
i += 1
continue
# Substitua:
# m_iof = RE_IOF_LINE.search(ln_clean)
m_iof = RE_IOF_LINE.search(line)
if m_iof:
valor = decomma(m_iof.group(1))
iof_postings.append(build(card, last_date or "", "Repasse de IOF em R$", valor, "IOF", ref_y, ref_m, iof_brl=valor))
stats["iof"] += 1
i += 1
continue
# if any(x in ln_clean.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
if any(x in line.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
mval = RE_BRL.search(line)
if mval:
valor = decomma(mval.group(0))
if valor != 0:
rows.append(build(card, last_date or "", line, valor, "ENCARGOS", ref_y, ref_m))
stats["encargos"] += 1
i += 1
continue
stats["regex_miss"] += 1
if verbose:
prev_line = lines[i-1] if i > 0 else ""
next_line = lines[i+1] if i+1 < len(lines) else ""
with open(f"{path.stem}_faltantes.txt", "a", encoding="utf-8") as f:
f.write(f"{i+1:04d}|{lines[i]}\n")
if prev_line:
f.write(f" [prev] {prev_line}\n")
if next_line:
f.write(f" [next] {next_line}\n")
i += 1
rows.extend(iof_postings)
# Filtro final antes do CSV
rows = [r for r in rows if r.get("post_date") and r.get("valor_brl") not in ("", None)]
stats["postings"] = len(rows)
return rows, stats

def log_block(tag, **kv):
logging.info("%s | %-8s", datetime.now().strftime("%H:%M:%S"), tag)
for k, v in kv.items():
logging.info(" %-12s: %s", k, v)

def main():
tracemalloc.start()
ap = argparse.ArgumentParser(); ap.add_argument("files", nargs="+"); ap.add_argument("-v", "--verbose", action="store_true")
a = ap.parse_args(); logging.basicConfig(level=logging.INFO, format="%(message)s")
total = Counter(); t0 = time.perf_counter()
for f in a.files:
p = Path(f); m = re.search(r"(20\d{2})(\d{2})", p.stem); ry, rm = (int(m.group(1)), int(m.group(2))) if m else (datetime.date.today().year, datetime.date.today().month)
log_block("START", v=__version__, file=p.name, sha=hashlib.sha1(p.read_bytes()).hexdigest()[:8])
rows, stats = parse_txt(p, ry, rm, a.verbose); total += stats
rows_dedup = rows
# Checagem de duplicatas por ledger_hash
seen_hashes = set()
dupes = 0
for r in rows:
if r["ledger_hash"] in seen_hashes:
print(f"[DUPLICATE] Linha duplicada: {r['desc_raw']} | {r['post_date']} | {r['valor_brl']}")
dupes += 1
else:
seen_hashes.add(r["ledger_hash"])
rows_dedup = rows # Mantém todas as linhas para rastreabilidade
kpi = Counter()
for r in rows_dedup:
if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS"): kpi['domestic'] += 1
elif r["categoria_high"] == "FX": kpi['fx'] += 1
elif r["categoria_high"] in ("SERVIÇOS",): kpi['services'] += 1
else: kpi['misc'] += 1
brl_dom = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO"))
brl_fx = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] == "FX")
brl_serv = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] == "SERVIÇOS")
neg_rows = sum(1 for r in rows_dedup if r.get("valor_brl") not in ("", None) and Decimal(str(r["valor_brl"])) < 0)
neg_sum = sum(Decimal(str(r["valor_brl"])) for r in rows_dedup if r.get("valor_brl") not in ("", None) and Decimal(str(r["valor_brl"])) < 0)
header_total = 20860.60
header_pagamentos = -21732.62
# --- NOVO: Métricas de referência extraídas do PDF/TXT ---
pdf_metrics = {
"Total da fatura anterior": 9232.62,
"Pagamentos efetuados": -21732.62,
"Saldo financiado": -12500.00,
"Lançamentos atuais": 33360.60,
"Total desta fatura": 20860.60,
"Nº de pagamentos 7117": 6,
"Valor total dos pagamentos": -21732.62,
"Valor do maior pagamento": -9232.62,
"Nº de compras domésticas": 78,
"Valor total compras domésticas": 7792.56,
"Nº de compras internacionais": 71,
"Valor total compras internacionais (BRL)": 18574.30,
"Valor total lançamentos internacionais (BRL)": 19202.05,
"Valor total IOF internacional": 627.75,
"Maior compra internacional": 2650.68,
"Menor compra internacional": 5.94,
"Nº de cartões diferentes": 4,
"Valor total de produtos/serviços": 293.53,
"Nº de ajustes negativos": 10,
"Valor total ajustes negativos": -0.92,
"Saldo calculado": 20860.60,
}
# --- NOVO: Métricas extraídas do CSV ---
# Pagamentos: só do ciclo atual (ignora o primeiro, que está em pagamento_fatura_anterior)
pagamentos_csv = [r for r in rows_dedup if r["categoria_high"] == "PAGAMENTO" and r.get("valor_brl") not in ("", None)]
pagamentos_ciclo = pagamentos_csv
# FX: só se campos de metadados estiverem preenchidos
fx_rows = [r for r in rows_dedup if r["categoria_high"] == "FX" and r.get("valor_orig") and r.get("moeda_orig") and r.get("fx_rate")]
csv_metrics = {
"Total da fatura anterior": sum(float(r.get("pagamento_fatura_anterior", 0) or 0) for r in rows_dedup),
"Pagamentos efetuados": sum(float(r["valor_brl"]) for r in pagamentos_ciclo),
"Saldo financiado": 0,
"Lançamentos atuais": sum(
float(r["valor_brl"]) for r in rows_dedup
if r["categoria_high"] not in ("PAGAMENTO", "AJUSTE") and (r.get("valor_brl") not in ("", None))
),
"Total desta fatura": sum(float(r["valor_brl"]) for r in rows_dedup if r.get("valor_brl") not in ("", None)),
"Nº de pagamentos 7117": len(pagamentos_ciclo),
"Valor total dos pagamentos": sum(float(r["valor_brl"]) for r in pagamentos_ciclo),
"Valor do maior pagamento": min((float(r["valor_brl"]) for r in pagamentos_ciclo), default=0),
"Nº de compras domésticas": sum(1 for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO")),
"Valor total compras domésticas": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO")),
"Nº de compras internacionais": len(fx_rows),
"Valor total compras internacionais (BRL)": sum(float(r["valor_brl"]) for r in fx_rows),
"Valor total lançamentos internacionais (BRL)": sum(float(r["valor_brl"]) for r in fx_rows) + sum(float(r.get("iof_brl", 0) or 0) for r in rows_dedup if r["categoria_high"] == "IOF"),
"Valor total IOF internacional": sum(float(r.get("iof_brl", 0) or 0) for r in rows_dedup if r["categoria_high"] == "IOF"),
"Maior compra internacional": max((float(r["valor_brl"]) for r in fx_rows), default=0),
"Menor compra internacional": min((float(r["valor_brl"]) for r in fx_rows), default=0),
"Nº de cartões diferentes": len(set(r["card_last4"] for r in rows_dedup)),
"Valor total de produtos/serviços": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] == "SERVIÇOS"),
"Nº de ajustes negativos": sum(1 for r in rows_dedup if r["categoria_high"] == "AJUSTE"),
"Valor total ajustes negativos": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] == "AJUSTE"),
"Saldo calculado": sum(float(r["valor_brl"]) for r in rows_dedup if r.get("valor_brl") not in ("", None)),
}
compare_metrics(pdf_metrics, csv_metrics)
log_block("TOTAL", Débitos=f"{brl_dom+brl_fx+brl_serv:,.2f}", Créditos=f"{neg_sum:,.2f}", Net=f"{brl_dom+brl_fx+brl_serv+neg_sum:,.2f}")
log_block("POSTINGS", rows=len(rows_dedup), dom=kpi["domestic"], fx=kpi["fx"], ajustes=kpi["ajuste"], pagamentos=kpi["pagamento"])
log_block("KPIS", miss=stats["regex_miss"], acc=f"{100*(stats['lines']-stats['hdr_drop']-stats['regex_miss'])/max(stats['lines']-stats['hdr_drop'],1):.1f}%")
out = p.with_name(f"{p.stem}_done.csv"); out.write_text("")
with out.open("w", newline="", encoding="utf-8") as fh:
w = csv.DictWriter(fh, fieldnames=SCHEMA); w.writeheader(); w.writerows(rows_dedup)
size_kb = out.stat().st_size // 1024; log_block("FILES", in_=p.name, out=f"{out.name} ({size_kb} KB)")
mem = tracemalloc.get_traced_memory()[1] // 1024 ** 2; log_block("MEM", peak=f"{mem} MB"); log_block("END", result="SUCCESS")
dur = time.perf_counter() - t0; eff_g = total["lines"] - total["hdr_drop"]; acc_g = 100 * (eff_g - total["regex_miss"]) / max(eff_g, 1)
log_block("SUMMARY", files=len(a.files), postings=total["postings"], miss=total["regex_miss"], acc=f"{acc_g:.1f}%", dur=f"{dur:.2f}s")

# Após parsing e antes de escrever no CSV:
rows_validos = [r for r in rows if r.get("post_date") and r.get("valor_brl") not in ("", None)]

debito_total = sum(
float(r["valor_brl"]) for r in rows_validos
if float(r["valor_brl"]) > 0 and r["categoria_high"] not in ("PAGAMENTO", "AJUSTE")
)
credito_total = sum(
float(r["valor_brl"]) for r in rows_validos
if float(r["valor_brl"]) < 0 and r["categoria_high"] in ("PAGAMENTO", "AJUSTE")
)
valor_total_fatura = debito_total + credito_total

print(f"[RECONCILIACAO] Débitos: {debito_total:.2f} | Créditos: {credito_total:.2f} | Total fatura: {valor_total_fatura:.2f}")

# Atualize as métricas do CSV para refletir os novos totais
csv_metrics["Débitos"] = debito_total
csv_metrics["Créditos"] = credito_total
csv_metrics["Saldo calculado"] = valor_total_fatura

if __name__ == "__main__":
main()

☎️#################################☎️

☎️#################################☎️

☎️#################################☎️

#!/usr/bin/env python3
# Untitled-2.py - Itaú Fatura TXT → CSV + Sanity Check (2025-05-30)
# Pipeline com comparação automática de métricas PDF vs CSV

import re
import csv
import argparse
import logging
import datetime
import tracemalloc
import time
import hashlib
from decimal import Decimal, ROUND_HALF_UP
from pathlib import Path
from collections import Counter, defaultdict
from datetime import datetime

__version__ = "0.12.0"
DATE_FMT_OUT = "%Y-%m-%d"
SCHEMA = [
"card_last4", "post_date", "desc_raw", "valor_brl",
"installment_seq", "installment_tot",
"valor_orig", "moeda_orig", "valor_usd", "fx_rate",
"iof_brl", "categoria_high", "merchant_city", "ledger_hash",
"pagamento_fatura_anterior"
]

# --- Regexes
RE_DATE = re.compile(r"(?P<d>\d{1,3})/(?P<m>\d{1,2})(?:/(?P<y>\d{4}))?")
RE_PAY_HDR = re.compile(r"Pagamentos efetuados", re.I)
RE_BRL = re.compile(r"-?\s*\d{1,3}(?:\.\d{3})*,\d{2}")
RE_PAY_LINE = re.compile(r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO(\s+EFETUADO)?\s+7117\s*[-\t ]+(?P<amt>-\s*[\d.,]+)\s*$", re.I)
RE_PAY_LINE_ANY = re.compile(
r"^(?P<date>\d{1,3}/\d{1,2}(?:/\d{4})?)\s+PAGAMENTO.*?(?P<amt>-?\s*[\d.,]+)\s*$", re.I
)
RE_FX_L2_TOL = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$")
# Nova regex para moedas variadas e tolerância a espaços extras
RE_FX_L2_TOL_ANY = re.compile(r"^(.+?)\s+([\d.,]+)\s+([A-Z]{3})\s+([\d.,]+)$", re.I)
RE_FX_BRL = re.compile(r"^(?P<date>\d{1,2}/\d{1,2})(?:/\d{4})?\s+(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<brl>[\d.,]+)$")
RE_IOF_LINE = re.compile(r"Repasse de IOF", re.I)
RE_FX_MAIN = re.compile(
r'^(?P<date>\d{2}/\d{2})\s+(?P<descr>.+?)\s+'
r'(?P<orig>-?\d{1,3}(?:\.\d{3})*,\d{2})\s+'
r'(?P<brl>-?\d{1,3}(?:\.\d{3})*,\d{2})$'
)
RE_DOLAR = re.compile(r'^D[óo]lar de Convers[ãa]o.*?(\d+,\d{4})')
RE_FX_L2 = re.compile(r"^(?P<city>.+?)\s+(?P<orig>[\d.,]+)\s+(?P<cur>[A-Z]{3})\s+(?P<usd>[\d.,]+)$")
RE_FX_RATE = re.compile(r"D[oó]lar de Convers[aã]o R\$ (?P<fx>[\d.,]+)")
RE_DOM = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+(?P<desc>.+?)\s+(?P<amt>[-\d.,]+)$")
RE_CARD = re.compile(r"final (\d{4})")
RE_INST = re.compile(r"(\d{1,2})/(\d{1,2})")
RE_INST_TXT = re.compile(r"\+\s*(\d+)\s*x\s*R\$")
RE_AJUSTE_NEG = re.compile(
r"(?P<date>\d{1,3}/\d{1,2}).*?ajuste.*?(?P<amt>-\s*\d+,\d{2})",
re.I
)
RE_ROUND = re.compile(r"^(?P<date>\d{1,3}/\d{1,2})\s+-(?P<amt>0,\d{2})$")
RE_DROP_HDR = re.compile(r"^(Total |Lançamentos|Limites|Encargos|Próxima fatura|Demais faturas|Parcelamento da fatura|Simulação|Pontos|Cashback|Outros lançamentos|Limite total de crédito|Fatura anterior|Saldo financiado|Produtos e serviços|Tarifa|Compras parceladas - próximas faturas)", re.I)
LEAD_SYM = ">@§$Z)_•*®«» "

# Regex para FX e pagamentos
FX_LINE1 = re.compile(r"^\d{2}/\d{2} (.+?) (\d{1,3}(?:\.\d{3})*,\d{2})$")
FX_LINE2 = re.compile(r"^(.+?) (\d{1,3}(?:\.\d{3})*,\d{2}) (EUR|USD|GBP|CHF) (\d{1,3}(?:\.\d{3})*,\d{2})$")
FX_RATE = re.compile(r"D[óo]lar de Convers[ãa]o R\$ (\d{1,3}(?:\.\d{3})*,\d{2})")
PAGAMENTO = re.compile(r"^\d{2}/\d{2} PAGAMENTO.*?7117.*?(-?\d{1,3}(?:\.\d{3})*,\d{2})$")

def decomma(x: str) -> Decimal:
return Decimal(re.sub(r"[^\d,\-]", "", x.replace(' ', '')).replace('.', '').replace(',', '.'))

def norm_date(date, ry, rm):
if not date: return ""
m = RE_DATE.match(date)
if not m: return date
d, mth, y = m.groups()
if not y: y = ry
if not mth: mth = rm
return f"{int(y):04}-{int(mth):02}-{int(d):02}"

def sha1(card, date, desc, valor_brl, installment_tot, categoria_high):
h = hashlib.sha1()
h.update(f"{card}|{date}|{desc}|{valor_brl}|{installment_tot}|{categoria_high}".encode("utf-8"))
return h.hexdigest()

def classify(desc, amt):
d = desc.upper()
if "7117" in d: return "PAGAMENTO"
if "AJUSTE" in d or (abs(amt) <= Decimal("0.30") and abs(amt) > 0): return "AJUSTE"
if any(k in d for k in ("IOF", "JUROS", "MULTA")): return "ENCARGOS"
mapping = [
("ACELERADOR", "SERVIÇOS"),
("PONTOS", "SERVIÇOS"),
("ANUIDADE", "SERVIÇOS"),
("SEGURO", "SERVIÇOS"),
("TARIFA", "SERVIÇOS"),
("PRODUTO", "SERVIÇOS"),
("SERVIÇO", "SERVIÇOS"),
# ... demais categorias já existentes ...
("SUPERMERC", "SUPERMERCADO"),
("FARMAC", "FARMÁCIA"),
("DROG", "FARMÁCIA"),
("PANVEL", "FARMÁCIA"),
("RESTAUR", "RESTAURANTE"),
("PIZZ", "RESTAURANTE"),
("BAR", "RESTAURANTE"),
("CAFÉ", "RESTAURANTE"),
("POSTO", "POSTO"),
("COMBUST", "POSTO"),
("GASOLIN", "POSTO"),
("UBER", "TRANSPORTE"),
("TAXI", "TRANSPORTE"),
("TRANSP", "TRANSPORTE"),
("PASSAGEM", "TRANSPORTE"),
("AEROPORTO", "TURISMO"),
("HOTEL", "TURISMO"),
("TUR", "TURISMO"),
("ENTRETENIM", "TURISMO"),
("ALIMENT", "ALIMENTAÇÃO"),
("IFD", "ALIMENTAÇÃO"),
("SAUD", "SAÚDE"),
("VEIC", "VEÍCULOS"),
("VEST", "VESTUÁRIO"),
("LOJA", "VESTUÁRIO"),
("MAGAZINE", "VESTUÁRIO"),
("EDU", "EDUCAÇÃO"),
("HOBBY", "HOBBY"),
("DIVERS", "DIVERSOS"),
]
for k, v in mapping:
if k in d: return v
if "EUR" in d or "USD" in d or "FX" in d: return "FX"
print(f"[CAT-SUSPEITA] {desc}")
return "DIVERSOS"

def build(card, date, desc, valor_brl, cat, ry, rm, **kv):
norm = norm_date(date, ry, rm) if date else ""
ledger = sha1(card, norm, desc, valor_brl, kv.get("installment_tot"), cat)
city = None
if cat == "FX" and "merchant_city" in kv and kv["merchant_city"]:
city = kv["merchant_city"]
elif cat == "FX" and " " in desc:
city = desc.split()[0].title()
# Remove merchant_city de kv se já está sendo passado explicitamente
kv = dict(kv)
if "merchant_city" in kv:
del kv["merchant_city"]
d = dict(card_last4=card, post_date=norm, desc_raw=desc, valor_brl=valor_brl,
categoria_high=cat, merchant_city=city, ledger_hash=ledger, **kv)
# Preenche campos obrigatórios vazios
for k in SCHEMA:
if k not in d:
d[k] = ""
# Logging de campos obrigatórios faltando
obrigatorios = ["card_last4", "post_date", "desc_raw", "valor_brl", "categoria_high", "ledger_hash"]
for k in obrigatorios:
if not d[k]:
print(f"[OBRIGATORIO-FALTANDO] {k} vazio em linha: {desc}")
return d

def is_prev_bill_payoff(descr: str, seq_no: int) -> bool:
"""
Retorna True se a linha é o 1º pagamento do bloco
ou contém keywords típicas de quitação da fatura anterior.
"""
kw = ('fatura anterior', 'ref.', 'refª', 'pagt anterior')
return seq_no == 0 or any(k in descr.lower() for k in kw)

# --- Sanitiser: remove Private Use Area glyphs (ícones) --------------------
def strip_pua(s: str) -> str:
return re.sub('[\ue000-\uf8ff]', '', s)

def clean(raw: str) -> str:
raw = strip_pua(raw)
raw = raw.lstrip(LEAD_SYM).replace("_", " ")
raw = re.sub(r"\s{2,}", " ", raw)
return raw.strip()

def compare_metrics(pdf_metrics, csv_metrics):
def emoji(ok): return "✅" if ok else "⚠️"
for key in pdf_metrics:
val_pdf = pdf_metrics[key]
val_csv = csv_metrics.get(key)
# PATCH: converte ambos para float se forem numéricos (float, int, Decimal)
if isinstance(val_pdf, (float, int, Decimal)) or isinstance(val_csv, (float, int, Decimal)):
try:
ok = abs(float(val_pdf) - float(val_csv or 0)) < 0.05
except Exception:
ok = False
else:
ok = val_pdf == val_csv
print(f"[CHECK] {key}: {val_pdf} (PDF) vs {val_csv} (CSV) {emoji(ok)}")

def parse_fx_chunk(chunk: list[str]):
"""
Aceita cluster 2-ou-3 linhas:
• Compra → Dólar (sem IOF)
• Compra → IOF → Dólar
• Compra → Dólar → IOF
Devolve dict com valores normalizados ou None.
"""
if len(chunk) < 2:
return None

main = RE_FX_MAIN.match(chunk[0])
if not main:
return None

iof_brl = Decimal('0')
rate_line = None
for ln in chunk[1:]:
if RE_IOF_LINE.search(ln):
m = RE_BRL.search(ln)
if m:
iof_brl = decomma(m.group(0))
elif RE_DOLAR.search(ln):
rate_line = ln

if not rate_line:
return None

fx_rate = Decimal(RE_DOLAR.search(rate_line).group(1).replace(',', '.'))
return {
"date": main.group('date'),
"descr": main.group('descr'),
"valor_orig": decomma(main.group('orig')),
"valor_brl": decomma(main.group('brl')),
"fx_rate": fx_rate,
"iof": iof_brl,
}

def parse_txt(path: Path, ref_y: int, ref_m: int, verbose=False):
rows, stats = [], Counter()
card = "0000"; iof_postings = []
lines = path.read_text(encoding="utf-8", errors="ignore").splitlines(); stats["lines"] = len(lines)
skip = 0
last_date = None
pagamento_seq = 0
seen_fx = set()
i = 0
while i < len(lines):
if skip:
skip -= 1
i += 1
continue
line = clean(lines[i])

# FX bloco: aceita 2 ou 3 linhas
fx_res = None
if i + 2 < len(lines):
fx_res = parse_fx_chunk([line, clean(lines[i+1]), clean(lines[i+2])])
consumed = 3
if not fx_res and i + 1 < len(lines):
fx_res = parse_fx_chunk([line, clean(lines[i+1])])
consumed = 2
if fx_res:
fx_key = (fx_res['descr'], fx_res['date'], fx_res['valor_brl'], fx_res['valor_orig'], fx_res['fx_rate'])
if fx_key in seen_fx:
print(f"[DUPLICATE] Duplicata legítima confirmada: {fx_res['descr']} | {fx_res['date']} | {fx_res['valor_brl']}")
else:
seen_fx.add(fx_key)
rows.append(build(
card, fx_res['date'], fx_res['descr'], fx_res['valor_brl'], "FX", ref_y, ref_m,
valor_orig=fx_res['valor_orig'], fx_rate=fx_res['fx_rate'], iof_brl=fx_res['iof']
))
stats["fx"] += 1
i += consumed
continue
# Pagamentos (regex abrangente)
mp = RE_PAY_LINE.match(line) or RE_PAY_LINE_ANY.match(line)
if mp and mp.group("amt"):
if is_prev_bill_payoff(line, pagamento_seq):
print("[PAGAMENTO-IGNORE] Pre-cycle payoff", line)
pagamento_seq += 1
i += 1
continue

val = decomma(mp.group("amt"))
if val >= 0:
print("[PAGAMENTO-ERR] Pagamento positivo ignorado:", line)
i += 1
continue

rows.append(build(card, mp.group("date"), "PAGAMENTO",
val, "PAGAMENTO", ref_y, ref_m))
stats["pagamento"] += 1
pagamento_seq += 1
i += 1
continue
elif mp:
print(f"[PAGAMENTO-ERR] Regex de pagamento não encontrou grupo 'amt' em: {line}")
i += 1
continue
# Parsing de compras domésticas (fallback)
if RE_DATE.match(line):
md = RE_DOM.match(line)
if md:
desc, amt = md.group("desc"), decomma(md.group("amt"))
if abs(amt) > 10000 or abs(amt) < 0.01:
print(f"[VALOR-SUSPEITO] {desc} {amt}")
re_parc = re.compile(r"(\d{1,2})\s*/\s*(\d{1,2})|(\d{1,2})\s*x\s*R\$|(\d{1,2})\s*de\s*(\d{1,2})", re.I)
ins = RE_INST.search(desc) or RE_INST_TXT.search(desc) or re_parc.search(desc)
if ins:
if ins.lastindex == 2:
seq, tot = int(ins.group(1)), int(ins.group(2))
elif ins.lastindex == 3:
seq, tot = int(ins.group(3)), None
elif ins.lastindex == 5:
seq, tot = int(ins.group(4)), int(ins.group(5))
else:
seq, tot = None, None
# Só aceita parcelas do ciclo atual
if tot and seq and seq > tot:
print(f"[PARCELA-ERR] Parcela fora do ciclo: {desc}")
i += 1
continue
else:
seq, tot = None, None
cat = classify(desc, amt)
if cat == "DIVERSOS":
print(f"[CAT-SUSPEITA] {desc}")
rows.append(build(card, md.group("date"), desc, amt, cat, ref_y, ref_m, installment_seq=seq, installment_tot=tot))
stats[cat.lower()] += 1
last_date = md.group("date")
i += 1
continue
# Substitua:
# m_iof = RE_IOF_LINE.search(ln_clean)
m_iof = RE_IOF_LINE.search(line)
if m_iof:
mval = RE_BRL.search(line)
if not mval:
i += 1
continue
valor = decomma(mval.group(0))
iof_postings.append(build(card, last_date or "", "Repasse de IOF em R$", valor, "IOF", ref_y, ref_m, iof_brl=valor))
stats["iof"] += 1
i += 1
continue
# if any(x in ln_clean.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
if any(x in line.upper() for x in ("JUROS", "MULTA", "IOF DE FINANCIAMENTO")):
mval = RE_BRL.search(line)
if mval:
valor = decomma(mval.group(0))
if valor != 0:
rows.append(build(card, last_date or "", line, valor, "ENCARGOS", ref_y, ref_m))
stats["encargos"] += 1
i += 1
continue
stats["regex_miss"] += 1
if verbose:
prev_line = lines[i-1] if i > 0 else ""
next_line = lines[i+1] if i+1 < len(lines) else ""
with open(f"{path.stem}_faltantes.txt", "a", encoding="utf-8") as f:
f.write(f"{i+1:04d}|{lines[i]}\n")
if prev_line:
f.write(f" [prev] {prev_line}\n")
if next_line:
f.write(f" [next] {next_line}\n")
i += 1
rows.extend(iof_postings)
# Filtro final antes do CSV
rows = [r for r in rows if r.get("post_date") and r.get("valor_brl") not in ("", None)]
stats["postings"] = len(rows)
return rows, stats

def log_block(tag, **kv):
logging.info("%s | %-8s", datetime.now().strftime("%H:%M:%S"), tag)
for k, v in kv.items():
logging.info(" %-12s: %s", k, v)

def main():
tracemalloc.start()
ap = argparse.ArgumentParser(); ap.add_argument("files", nargs="+"); ap.add_argument("-v", "--verbose", action="store_true")
a = ap.parse_args(); logging.basicConfig(level=logging.INFO, format="%(message)s")
total = Counter(); t0 = time.perf_counter()
for f in a.files:
p = Path(f); m = re.search(r"(20\d{2})(\d{2})", p.stem); ry, rm = (int(m.group(1)), int(m.group(2))) if m else (datetime.date.today().year, datetime.date.today().month)
log_block("START", v=__version__, file=p.name, sha=hashlib.sha1(p.read_bytes()).hexdigest()[:8])
rows, stats = parse_txt(p, ry, rm, a.verbose); total += stats
rows_dedup = rows
# Checagem de duplicatas por ledger_hash
seen_hashes = set()
dupes = 0
for r in rows:
if r["ledger_hash"] in seen_hashes:
print(f"[DUPLICATE] Linha duplicada: {r['desc_raw']} | {r['post_date']} | {r['valor_brl']}")
dupes += 1
else:
seen_hashes.add(r["ledger_hash"])
rows_dedup = rows # Mantém todas as linhas para rastreabilidade
kpi = Counter()
for r in rows_dedup:
if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS"): kpi['domestic'] += 1
elif r["categoria_high"] == "FX": kpi['fx'] += 1
elif r["categoria_high"] in ("SERVIÇOS",): kpi['services'] += 1
else: kpi['misc'] += 1
brl_dom = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO"))
brl_fx = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] == "FX")
brl_serv = sum(r["valor_brl"] for r in rows_dedup if r["categoria_high"] == "SERVIÇOS")
neg_rows = sum(1 for r in rows_dedup if r.get("valor_brl") not in ("", None) and Decimal(str(r["valor_brl"])) < 0)
neg_sum = sum(Decimal(str(r["valor_brl"])) for r in rows_dedup if r.get("valor_brl") not in ("", None) and Decimal(str(r["valor_brl"])) < 0)
header_total = 20860.60
header_pagamentos = -21732.62
# --- NOVO: Métricas de referência extraídas do PDF/TXT ---
pdf_metrics = {
"Total da fatura anterior": 9232.62,
"Pagamentos efetuados": -21732.62,
"Saldo financiado": -12500.00,
"Lançamentos atuais": 33360.60,
"Total desta fatura": 20860.60,
"Nº de pagamentos 7117": 6,
"Valor total dos pagamentos": -21732.62,
"Valor do maior pagamento": -9232.62,
"Nº de compras domésticas": 78,
"Valor total compras domésticas": 7792.56,
"Nº de compras internacionais": 71,
"Valor total compras internacionais (BRL)": 18574.30,
"Valor total lançamentos internacionais (BRL)": 19202.05,
"Valor total IOF internacional": 627.75,
"Maior compra internacional": 2650.68,
"Menor compra internacional": 5.94,
"Nº de cartões diferentes": 4,
"Valor total de produtos/serviços": 293.53,
"Nº de ajustes negativos": 10,
"Valor total ajustes negativos": -0.92,
"Saldo calculado": 20860.60,
}
# --- NOVO: Métricas extraídas do CSV ---
# Pagamentos: só do ciclo atual (ignora o primeiro, que está em pagamento_fatura_anterior)
pagamentos_csv = [r for r in rows_dedup if r["categoria_high"] == "PAGAMENTO" and r.get("valor_brl") not in ("", None)]
pagamentos_ciclo = pagamentos_csv
# FX: só se campos de metadados estiverem preenchidos
fx_rows = [r for r in rows_dedup if r["categoria_high"] == "FX" and r.get("valor_orig") and r.get("moeda_orig") and r.get("fx_rate")]
csv_metrics = {
"Total da fatura anterior": sum(float(r.get("pagamento_fatura_anterior", 0) or 0) for r in rows_dedup),
"Pagamentos efetuados": sum(float(r["valor_brl"]) for r in pagamentos_ciclo),
"Saldo financiado": 0,
"Lançamentos atuais": sum(
float(r["valor_brl"]) for r in rows_dedup
if r["categoria_high"] not in ("PAGAMENTO", "AJUSTE") and (r.get("valor_brl") not in ("", None))
),
"Total desta fatura": sum(float(r["valor_brl"]) for r in rows_dedup if r.get("valor_brl") not in ("", None)),
"Nº de pagamentos 7117": len(pagamentos_ciclo),
"Valor total dos pagamentos": sum(float(r["valor_brl"]) for r in pagamentos_ciclo),
"Valor do maior pagamento": min((float(r["valor_brl"]) for r in pagamentos_ciclo), default=0),
"Nº de compras domésticas": sum(1 for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO")),
"Valor total compras domésticas": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] in ("ALIMENTAÇÃO", "SAÚDE", "VESTUÁRIO", "VEÍCULOS", "FARMÁCIA", "SUPERMERCADO", "POSTO", "RESTAURANTE", "TURISMO")),
"Nº de compras internacionais": len(fx_rows),
"Valor total compras internacionais (BRL)": sum(float(r["valor_brl"]) for r in fx_rows),
"Valor total lançamentos internacionais (BRL)": sum(float(r["valor_brl"]) for r in fx_rows) + sum(float(r.get("iof_brl", 0) or 0) for r in rows_dedup if r["categoria_high"] == "IOF"),
"Valor total IOF internacional": sum(float(r.get("iof_brl", 0) or 0) for r in rows_dedup if r["categoria_high"] == "IOF"),
"Maior compra internacional": max((float(r["valor_brl"]) for r in fx_rows), default=0),
"Menor compra internacional": min((float(r["valor_brl"]) for r in fx_rows), default=0),
"Nº de cartões diferentes": len(set(r["card_last4"] for r in rows_dedup)),
"Valor total de produtos/serviços": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] == "SERVIÇOS"),
"Nº de ajustes negativos": sum(1 for r in rows_dedup if r["categoria_high"] == "AJUSTE"),
"Valor total ajustes negativos": sum(float(r["valor_brl"]) for r in rows_dedup if r["categoria_high"] == "AJUSTE"),
"Saldo calculado": sum(float(r["valor_brl"]) for r in rows_dedup if r.get("valor_brl") not in ("", None)),
}
compare_metrics(pdf_metrics, csv_metrics)
log_block("TOTAL", Débitos=f"{brl_dom+brl_fx+brl_serv:,.2f}", Créditos=f"{neg_sum:,.2f}", Net=f"{brl_dom+brl_fx+brl_serv+neg_sum:,.2f}")
log_block("POSTINGS", rows=len(rows_dedup), dom=kpi["domestic"], fx=kpi["fx"], ajustes=kpi["ajuste"], pagamentos=kpi["pagamento"])
log_block("KPIS", miss=stats["regex_miss"], acc=f"{100*(stats['lines']-stats['hdr_drop']-stats['regex_miss'])/max(stats['lines']-stats['hdr_drop'],1):.1f}%")
out = p.with_name(f"{p.stem}_done.csv"); out.write_text("")
with out.open("w", newline="", encoding="utf-8") as fh:
w = csv.DictWriter(fh, fieldnames=SCHEMA); w.writeheader(); w.writerows(rows_dedup)
size_kb = out.stat().st_size // 1024; log_block("FILES", in_=p.name, out=f"{out.name} ({size_kb} KB)")
mem = tracemalloc.get_traced_memory()[1] // 1024 ** 2; log_block("MEM", peak=f"{mem} MB"); log_block("END", result="SUCCESS")
dur = time.perf_counter() - t0; eff_g = total["lines"] - total["hdr_drop"]; acc_g = 100 * (eff_g - total["regex_miss"]) / max(eff_g, 1)
log_block("SUMMARY", files=len(a.files), postings=total["postings"], miss=total["regex_miss"], acc=f"{acc_g:.1f}%", dur=f"{dur:.2f}s")

# Após parsing e antes de escrever no CSV:
rows_validos = [r for r in rows if r.get("post_date") and r.get("valor_brl") not in ("", None)]

debito_total = sum(
float(r["valor_brl"]) for r in rows_validos
if float(r["valor_brl"]) > 0 and r["categoria_high"] not in ("PAGAMENTO", "AJUSTE")
)
credito_total = sum(
float(r["valor_brl"]) for r in rows_validos
if float(r["valor_brl"]) < 0 and r["categoria_high"] in ("PAGAMENTO", "AJUSTE")
)
valor_total_fatura = debito_total + credito_total

print(f"[RECONCILIACAO] Débitos: {debito_total:.2f} | Créditos: {credito_total:.2f} | Total fatura: {valor_total_fatura:.2f}")

# Atualize as métricas do CSV para refletir os novos totais
csv_metrics["Débitos"] = debito_total
csv_metrics["Créditos"] = credito_total
csv_metrics["Saldo calculado"] = valor_total_fatura

if __name__ == "__main__":
main()


